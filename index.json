[{"content":"I am a Computer Science graduate student at the University of Massachusetts - Amherst. I was previously affiliated with Intel, building OpenFL (a Linux Foundation project) and Intel Tiber Secure Federated AI - a Federated Learning Service to train models on private, decentralized data with confidentiality.\nReach me at karanshah1698 [at] gmail [dot] com.\n","date":null,"permalink":"https://masterskepticista.github.io/","section":"","summary":"","title":""},{"content":" Most \u0026lsquo;preferences\u0026rsquo; we defend are not preferences. They are conclusions reached before exposure. We mistake unfamiliarity for dislike, speed for judgment, and narrative for experience.\nEfficiency trap #Premature opinions feel efficient because they collapse uncertainty quickly. They reduce cognitive load by replacing curiosity with closure.\nA distinction has to be made about opinions that are \u0026lsquo;refined\u0026rsquo; through experience: that is judgment. Judgment is expensive. It is earned through contact with reality: trial, error, and revision.\nOpinions let us move fast. They create an illusion of clarity.\nLost futures #The cost of forming opinions before exposure is in missed optionality.\nYou sample less, which means fewer chances to discover fit, leverage, or upside. You miss second-order benefits from paths you never entered. You spend energy defending identities you did not earn. We do not pause-and-reflect because opinion is socially cheaper than exploration.\nImagination tax #Social settings reward immediacy. Silence reads as ignorance, and hesitation reads as weakness. So we opine early. We inflate the cost it takes to actually gather reasonable amount of information or exposure to the thing. Some may find themselves filling those experience gaps with identity narratives: “I don’t do X”, “I prefer Y”.\nIn practice, it takes far less effort to learn \u0026lsquo;just enough\u0026rsquo; about what is being asked of, and then build an opinion on it, or, to empathize with contrasting viewpoints of your peers about something.\nWe overestimate the cost of exposure, and underestimate the cost of guessing.\nExperience collapses false complexity #Many problems feel complex only from a distance. Once we engage with reality, simple constraints and real trade-offs are revealed. None of which are visible from imagination alone. Exposure builds experience; and experience invalidates shaky beliefs.\nThe takeaway (I think) is to have opinions, but treat them as debt. If you haven’t paid for one with exposure, curiosity, or effort; don’t carry it forward.\nDefault to gathering experience. Let judgment come later.\nIf you don\u0026#39;t have an opinion, resist the pressure to have one.\n\u0026mdash; Nassim Nicholas Taleb (@nntaleb) May 12, 2024 ","date":"7 January 2026","permalink":"https://masterskepticista.github.io/posts/early-opinions/","section":"Posts","summary":"Some \u0026ldquo;preferences\u0026rdquo; masquerade as lack of exposure.","title":"An Opinion on opinions"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/misc/","section":"Tags","summary":"","title":"Misc"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/avx512/","section":"Tags","summary":"","title":"Avx512"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/matmul/","section":"Tags","summary":"","title":"Matmul"},{"content":" Work-in-progress. Code available here. This is a worklog on optimizing a single-precision generalized matrix-multiply (GEMM) kernel in C to land close to Intel MKL-CBLAS performance.\nAlgorithmica: Matrix Multiplication Advanced Matrix Multiplication on Multi-Core Processors George Hotz | Programming | can you multiply a matrix? (noob lesson) Introduction #Let us start by describing the pointwise operation:\nvoid gemm_naive(float *C, const float *A, const float *B, int M, int N, int K) { memset(C, 0, M * N * sizeof(float)); for (int i = 0; i \u0026lt; M; i++) { for (int j = 0; j \u0026lt; N; j++) { for (int k = 0; k \u0026lt; K; k++) { C[i * N + j] += A[i * K + k] * B[k * N + j]; } } } } It takes ~1.2 seconds for this kernel to multiply two 1000-size square matrices. This is absurdly slow for a CPU of this day and age. Before we start optimizing it, we need to take stock of where we are and what the prize is.\nRoofline Analysis #System specs:\nIntel Xeon [Sapphire Rapids] 8488C @ 2.5GHz, 2 vCPUs Cache L1d: 48 KB/core | L2: 2 MB/core | L3: 105 MB/shared ISA support: AVX | AVX-2 | AVX-512 Microarchitecture: Golden Cove1 4GB Memory, 10GB/s STREAM bandwidth (measured using mbw) Ubuntu 24.04 LTS Performance of Generalized Matrix Multiply (GEMM) \u0026lsquo;kernels\u0026rsquo; will be measured in FLOP/s (floating point operations per second); common choice in many ML/HPC workloads. How many operations? GEMM involves K dot products to furnish each element of result matrix C.\n$$ A^{M \\times K} \\times B^{K \\times N} = 2 \\cdot M \\cdot N \\cdot (K - 1) \\approx 2 \\cdot MNK $$\nWe apply a scaling factor of 2 because we count multiply and adds as two separate ops. For equal matrix dimensions, this is roughly \\(2N^3\\) FLOPs. Assuming single precision floats, each input matrix A, B of size \\(4 \\cdot N^2\\) bytes will be read once. Output matrix of size \\(4 \\cdot N^2\\) bytes will be read and written back to memory. Therefore any kernel will read/write at least \\(4 \\cdot (4 \\cdot N^2) \\) bytes.\nIt is clear that total math ops grow faster than total memory read/writes. This ratio of floating point ops per byte of data moved is called the arithmetic intensity of an operation. For GEMM, arithmetic intensity \\(\\alpha\\) is:\n$$ \\alpha = \\frac{\\text{ \\char\u0026quot;0023 operations }}{\\text{ \\char\u0026quot;0023 bytes transferred }} = \\frac{2N^3}{16N^2} = \\frac{N}{8} \\text{ FLOPs/byte } $$\nThis means that as matrix sizes grow, GEMM operation becomes compute-bound. In fact, if we know the compute and memory bandwidth of a machine, we can find the machine\u0026rsquo;s \u0026lsquo;ridge point\u0026rsquo;. Any kernel that uses less FLOPs/byte from the ridge point is said to be memory-bound, and vice versa.\nOur CPU has two FMA (fused multiply-add) units that can operate on 256/512-bit width vectors. In single precision, this means 8/16 floats per vector. Each core also has 16/32 registers that are accessible in a single clock cycle. Registers sit on top of the memory hierarchy. FMA units have a latency of 4 clock cycles, and a throughput of 2 IPC2. Well-designed kernels and compilers can saturate FMA throughput, so the first-dispatch latency is not relevant. This gives us enough information to calculate the compute bandwidth:\n$$ 2 \\text{ ops } \\times 2 \\text{ IPC } \\times 16 \\text{ floats/cycle } \\times 2.5 \\text{ GHz } = 160 \\text{ GFLOP/s } $$\nIf the compiler chooses to use 256-bit width vectors, our performance ceiling halves to 80 GFLOP/s. The memory bandwidth on our setup is 10GB/s per thread from a simple mbw benchmark. Therefore, the ridge point \\(\\gamma\\) of this CPU is:\n$$ \\gamma = \\frac{\\text{compute BW}}{\\text{memory BW}} = 16 \\text{ FLOPs/byte } $$\nIn practice, the ridge point is higher due to cache effects, branching, and other instruction overheads. Nonetheless our GEMM operation can become compute-bound only beyond the ridge point of \\( N \\gt 128\\).\nGEMM example in the naive kernel above accounts for \\(2 \\times 10^9\\) floating point operations. We barely exceed ~1% of the CPU\u0026rsquo;s arithmetic capacity. As we will see, we do a poor job keeping the FMA units busy; the CPU spends majority time in load/stores. Memory Layout #As mentioned earlier, our arrays store floats in a row-major order, i.e., elements of a row are laid out consecutively. CPUs fetch contiguous blocks of memory (called a cache line) in the hope that consecutive memory elements will be needed for further processing. If a computation does not utilize all items in a cache line optimally, CPU cycles are wasted.\nThis gives us a couple of observations:\nInnermost loop iterates the fastest, over dimension K. Array A[M * K] has K columns, with each element A[i][k] consecutively laid out in memory. Therefore, iteration over K is cache-friendly. Array B[K * N] has K rows, each element B[k][j] requires jumping an entire row of N elements in memory. This results in a poor cache utilization. Array C[M * N] has j as the fastest moving dimension, i.e., the second loop. It is consecutively laid out in memory, and is cache-friendly. Kernel 1: Loop-reorder #Iterating over rows of B is the problem. Notice that the nested for-loops are order-independent, and array C does not depend on dimension K. Therefore, we can reorder the loops such that iterating over K dimension is slower, and hence less costly for B.\n1/** Basic loop-reordered, pointwise GEMM kernel. */ 2void gemm_loop_reorder(float* __restrict C, 3 const float* __restrict A, 4 const float* __restrict B, 5 int M, 6 int N, 7 int K) { 8 memset(C, 0, M * N * sizeof(float)); 9 for (int i = 0; i \u0026lt; M; i++) { 10 for (int k = 0; k \u0026lt; K; k++) { 11 for (int j = 0; j \u0026lt; N; j++) { 12 C[i * N + j] += A[i * K + k] * B[k * N + j]; 13 } 14 } 15 } 16} By swapping j \u0026lt;-\u0026gt; k, we retain cache-friendliness of A and C, while reusing the element B[k][j] for N iterations before incurring a cache miss. We still incur the same number of misses. We are simply amortizing the cost of each cache-miss by reusing the fetched element as long as possible.\nSGEMM Benchmark On small matrices, this simple tweak boosts our GFLOP/s by 10-25x, saturating on the lower end as matrices grow large. What explains this jump? Can performance be sustained over large matrices?\nImplicit Vectorization #Even though our loop-reordered kernel defines scalar operations, the order of loop enables the compiler (with -O2 flag) to fuse scalar operations into vector FMA (fused-multiply-add) instructions. We can see this in the disassembly of our kernel.\n.LBB0_19: vmovups ymm2, ymmword ptr [rsi + 4*r10] vmovups ymm3, ymmword ptr [rsi + 4*r10 + 32] vmovups ymm4, ymmword ptr [rsi + 4*r10 + 64] vmovups ymm5, ymmword ptr [rsi + 4*r10 + 96] vfmadd213ps ymm2, ymm1, ymmword ptr [r14 + 4*r10 - 64] vfmadd213ps ymm3, ymm1, ymmword ptr [r14 + 4*r10 - 32] vfmadd213ps ymm4, ymm1, ymmword ptr [r14 + 4*r10] vfmadd213ps ymm5, ymm1, ymmword ptr [r14 + 4*r10 + 32] vmovups ymmword ptr [r14 + 4*r10 - 64], ymm2 vmovups ymmword ptr [r14 + 4*r10 - 32], ymm3 vmovups ymmword ptr [r14 + 4*r10], ymm4 vmovups ymmword ptr [r14 + 4*r10 + 32], ymm5 This core has three kinds of vector registers: 128-wide xmm (first appeared as SSE3), 256-wide ymm (the classic AVX-2 intrinsics) and 512-wide zmm (new, AVX-512 intrinsics) registers. There are 16 ymm registers, each of which can hold 8 single-precision floats, and 32 zmm registers, each of which can hold 16 single-precision floats. Both xmm and ymm registers are a subset of the full 512-wide zmm registers to maintain backward compatibility.\nWhen ymm registers are used, the performance ceiling halves to 80 GFLOP/s since half of the maximum possible vector width is wasted on every clock. On small matrices, this loop-reordered kernel is an order of magnitude faster because the active blocks fit within the L2 cache boundary. As the matrix size grows, performance plateaus until active blocks fit L3. For even larger matrices, the active blocks exceed cache boundary, and require multiple read/writes into the main memory.\nKernel 2: Cache blocking #Cache size is limited. As matrix dimensions grow, there is a possibility of older cache lines being \u0026rsquo;evicted\u0026rsquo; to fetch elements for the next iteration. This leads to wasteful load/stores and lower arithmetic intensity for large matrix sizes. We solve this by slicing each of the three dimensions into \u0026rsquo;tiles\u0026rsquo;, and executing smaller, cache-friendly matrix multiplies on those tiles. Tuned how?\nTiling 1/** Cache-blocking across dimensions. */ 2#define TILE_K 128 3#define TILE_N 2048 4#define TILE_M 1024 5 6void gemm_cache_blocking(float* __restrict C, 7 const float* __restrict A, 8 const float* __restrict B, 9 int M, 10 int N, 11 int K) { 12 memset(C, 0, sizeof(float) * M * N); 13 14 // Tile across each dimension 15 for (int i = 0; i \u0026lt; M; i += TILE_M) { 16 const int mc = min(TILE_M, M - i); 17 for (int k = 0; k \u0026lt; K; k += TILE_K) { 18 const int kc = min(TILE_K, K - k); 19 for (int j = 0; j \u0026lt; N; j += TILE_N) { 20 const int nc = min(TILE_N, N - j); 21 22 // Update partials on each tile 23 for (int ir = 0; ir \u0026lt; mc; ir++) { 24 for (int p = 0; p \u0026lt; kc; p++) { 25 for (int jc = 0; jc \u0026lt; nc; jc++) { 26 C[(i + ir) * N + (j + jc)] += 27 A[(i + ir) * K + (k + p)] * B[(k + p) * N + (j + jc)]; 28 } 29 } 30 } 31 } 32 } 33 } 34} With cache-blocking, performance is consistent across all matrix sizes. The disassembly of this kernel is same as before. This is expected because the same instructions now run on \u0026rsquo;tiles\u0026rsquo; of matrices.\nSGEMM Benchmark Performance ceiling #So our kernel uses 256-bit FMAs, and cache-blocking to sustain GFLOP/s. Recall from our roofline analysis, the performance ceiling is 80 GFLOP/s. To understand the reason behind saturation at 40 GFLOP/s, review the disassembly:\nFrom the Golden Cove microarchitecture, we find the following uOp capacities:\nOp Capacity (per cycle) Requirement Cycles Loads \\(3 \\times 256\\) \\(8 \\times 256\\) \\(2.67\\) Stores \\(2 \\times 256\\) \\(4 \\times 256\\) \\(2\\) FMAs \\(2\\) \\(4\\) \\(2\\) A 32-bit scalar from A is broadcasted to ymm1 and reused for the entire iteration. The load cost is negligible compared to the rest, hence ignored in calculations. Loads take approximately 2.67 cycles. FMAs execute as soon as the operands are ready, and hence the load ops \u0026lsquo;mask\u0026rsquo; the 2 cycles consumed by FMAs. Stores take 2 cycles after FMAs retire. So the percentage of \u0026lsquo;useful\u0026rsquo; multiply-add work:\n$$ \\frac{2 \\text{ FMA}}{2.67 \\text{ loads } + 2 \\text{ stores}} = \\frac{2}{4.67} \\approx 0.43 $$\nIf FMA widths are 256-bit as in the disassembly, our performance ceiling with this kernel is \\(0.43 \\times 80 = 34.4 \\text{ GFLOP/s}\\). This matches our expected GFLOP/s from the kernel.\nForcing 512-bit vector widths #We can supply a compiler flag -mprefer-vector-width=512. Our requirements now look as follows:\nOp Capacity (per cycle) Requirement Cycles Loads \\(2 \\times 512\\) \\(8 \\times 512\\) \\(4\\) Stores \\(1 \\times 512\\) \\(4 \\times 512\\) \\(4\\) FMAs \\(2\\) \\(4\\) \\(2\\) The percentage of \u0026lsquo;useful\u0026rsquo; multiply-add work:\n$$ \\frac{2 \\text{ FMA}}{4 \\text{ loads } + 4 \\text{ stores}} = \\frac{2}{8} \\approx 0.25 $$\nOur performance ceiling with this flag is \\(0.25 \\times 160 = 40 \\text{ GFLOP/s}\\). It is a marginal improvement, because golden cove supports 2 loads/cycle when fetching 512-bit memory (compared to 3 loads/cycle for 256-bits). In practice, this flag gives us a very close GFLOP/s to what we predict. Neat!\nKernel 3: Outer Product #So far we have been looking at matrix multiplication as repeated dot products between rows of A and columns of B: $$ C_{ij} = \\sum_{k=1}^K A_{ik} \\cdot B_{kj} $$\nDot products are inefficient on hardware for the following reasons:\nFrequent Load/Stores for C: Tiles of C are read and written repeatedly. This is clear from our disassembly analysis. The useful FMA work is capped at 43%. Poor Register Utilization: Registers are the fastest to access in the memory hierarchy. Vector intrinsics on modern cores like Golden Cove have 16 vector registers (32 in AVX-512). The dot-product loop uses about 6-7 registers for temporary accumulations. Arithmetic Intensity: GEMM gets more compute intense with size. Our current implementation is load/store bound at large sizes. We need to amortize the cost of load/stores with more arithmetic work. Matrix-multiply as an outer product #Matrix multiply can be rewritten as a cumuluative outer-product between columns of A and rows of B: $$ C = A \\times B = \\sum_{k=0}^{K-1} A_{:,k} \\otimes B_{k,:} $$\nHere:\n\\(A_{:,k}\\) is the \\(k\\)-th column of A (an \\(M \\times 1\\) vector). \\(B_{k,:}\\) is the \\(k\\)-th row of B (a \\(1 \\times N\\) vector). Their outer product (\\(\\otimes\\)) produces an \\(M \\times N\\) matrix where each element is \\(A_{i,k} \\cdot B_{k,j}\\). Summing these over all \\(k\\) gives the full \\(C\\).\nThis is algebraically identical to the dot-product view but shifts the focus: instead of accumulating inward along \\(k\\) for each fixed \\((i,j)\\), we are broadcasting outward from each \\(k\\), adding a full \u0026ldquo;layer\u0026rdquo; to \\(C\\) at a time.\nWhat motivates this reformulation?\nRegister Reuse: In the outer-product view, we can load slices of \\(A_{:,k}\\) and \\(B_{k,:}\\) into registers, compute their outer product, and accumulate it directly into a register-resident tile of \\(C\\). Registers are plentiful (16 YMMs can hold 128 floats total), so we can \u0026ldquo;block\u0026rdquo; a small \\(\\text{MR} \\times \\text{NR}\\) tile of \\(C\\) using multiple ZMMs. Load/Store Amortization: After several updates over \\(k\\), we store the \\(\\text{MR} \\times \\text{NR}\\) tile of \\(C\\) back to memory. This amortizes load/store costs over more FMAs. Higher Arithmetic Intensity: By accumulating multiple outer products in registers, the ratio of computations to memory accesses increases. Outer Product using Registers #CPUs do not have an intrinsic for vector outer product, which means we need to compute one iteratively using vector FMAs.\nConsider loading \\(\\text{MR}\\) scalars from \\(A\\) across the column, and \\(\\text{NR}\\) scalars from \\(B\\) across the row.\nYou may (rightly) wonder that accesses across \\(A\\) are not cache-friendly. In practice, we transpose a tile of A into a buffer, which gets passed into the outer-product microkernel. Transposed A is cache-friendly and reuses the same for K outer products. Check code for details. We iteratively broadcast + FMA each of the scalars from \\(A\\) to vectors of \\(B\\), cumulatively storing the result in an \\(\\text{MR} \\times \\text{NR}\\) register tile of \\(C\\).\nOptimal Tile Sizes #When using YMM vector registers, we have a limit of 16. The vectors we load from \\(B\\) of size \\(\\text{NR}\\) must be a multiple of 8. Hence \\(B\\) vector will use \\(\\text{NR}/8\\) registers. Each scalar from \\(A\\) broadcasted into a vector uses 1 register, ipso facto \\(\\text{MR} \u0026gt; 1\\). The \\(C\\) accumulator of size is register resident, requiring \\(\\text{MR} \\times \\text{NR}/8\\) registers. Therefore, we must satisfy the inequality:\n$$ \\text{MR} \\cdot \\frac{\\text{NR}}{8} + \\frac{\\text{NR}}{8} + 1 \\leq 16 $$\nMR NR YMM Register Count Loads per iteration (bytes) FLOPs per iteration FLOPs/byte Remark 1 56 15 228 112 0.491 2 40 16 168 160 0.952 4 24 16 112 192 1.714 6 16 15 88 192 2.182 14 8 16 88 224 2.545 While many pairs of values satisfy this inequality, and it is worth testing different values, we will go with a well-tested BLAS GEMM micro-kernel of size \\(6 \\times 16\\).\nPopping the Hood on Golden Cove, by Chester Lam\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nInstructions per clock. Here it means that both FMA units can dispatch in parallel (ref).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStreaming SIMD Extensions debuted with Pentium-III.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"14 July 2025","permalink":"https://masterskepticista.github.io/posts/sgemm/","section":"Posts","summary":"Within 92% of Intel MKL.","title":"Optimizing a GEMM from first principles"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/sgemm/","section":"Tags","summary":"","title":"Sgemm"},{"content":"Personal projects, code snippets and/or notebooks.\n","date":null,"permalink":"https://masterskepticista.github.io/code/","section":"Code","summary":"","title":"Code"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/cuda/","section":"Tags","summary":"","title":"Cuda"},{"content":"Reduce operations are common in HPC applications. Put simply, a reduce operation combines all elements of an array into a single value through either sum, min, max, product, etc.\nReduce operations are embarrasingly parallel1, which makes them a great candidate to be run on GPUs.\nThis post will walk through a series of optimizations2 to iteratively obtain maximum device throughput.\nCode is available on GitHub.\n# Kernel Bandwidth (GB/s) Relative to jnp.sum 1 Vector Loads 9.9 1.1% 2 Tree Reduction 223 24.7% 3 Non-divergent Threads 317 36.3% 4 Sequential Addressing 331 38.0% 5 Reduce on First Load 618 70.9% 6 Warp Unrolling 859 98.6% 0 jnp.sum reference 871 100% Roofline Model #We first calculate the ridge point \\( \\gamma \\) of our RTX3090 GPU3 (i.e., minimum floating point operations that must be carried out on each byte of data, to hit peak FLOP/s a machine is capable of) as the ratio between compute and memory bandwidth. Note that in practice, ridge point is higher than this number because of cache effects, possible branching, and instruction overhead.\n$$ \\gamma = \\frac{\\text{compute BW}}{\\text{memory BW}} = \\frac{35600}{936} = 38 \\text{ FLOPs/byte} $$\nOur bench problem is to compute the sum of elements of a vector with N single precision floats. A reduce operation reads each element of the array at least once; about \\(4\\cdot N\\) bytes transferred, while performing \\(N-1 \\approx N\\) adds (or multiply/min/max depending on the type of reduction). Arithmetic intensity, \\( \\alpha \\), is defined as:\n$$ \\alpha = \\frac{\\text{ops}}{\\text{byte}} = \\frac{N-1}{4 \\cdot N} \\approx 0.25 \\text{ FLOPs/byte} $$\nArithmetic intensity \\(\\alpha \\lt\\lt \\gamma\\): it is severely memory bound. We can estimate the runtime based on peak compute and memory throughput values:\n\\(N\\) single precision reads at 936 GB/s = 0.136 ms \\(N\\) single precision adds at 35.6 TFLOPS = 0.0036 ms The theoretical minimum time for this operation is 0.136 + 0.0036 = 0.1396 ms. Since CUDA does not provide a built-in reduce_sum primitive, we will use jax.numpy.sum as a reference. jnp.sum completes in 0.15 ms, achieving 871 GB/s effective bandwidth.\nComplexity Analysis #Brent\u0026rsquo;s theorem is how we compare efficiency across different parallel algorithms. For a parallel algorithm, we can calculate:\n\\(W\\): work, the total number of operations if run serially. \\(S\\): step complexity, or the serialization cost when certain operations cannot be parallelized. \\(P\\): number of parallel processors. Given this, time complexity with \\(P\\) parallel processors is defined as:\n$$ T_p \\le \\frac{W}{P} + S $$\nWe can derive a couple of interesting bounds which will be helpful later:\n\\(T_\\infty = S\\): Infinite parallelism boils down to the time taken in sequential operations. \\(T_1 = W\\): Single processor time is when the total work is performed sequentially. \\(T_1 / T_p\\): Speedup when using \\(P\\) processors. \\(W / S\\): Parallelism achievable in the algorithm. For the reduce_sum operation, work complexity \\(W = N_{reads} + (N-1)_{adds}\\) is \\(O(N)\\).\nBaseline #Jensen would hate me for using GPUs to sum \\(N\\) elements using atomic operations. But this \u0026ldquo;serial\u0026rdquo; operation serves as a baseline for all parallelism we will achieve later.\n__global__ void reduce_sum_kernel1(float *out, const float *arr, int N) { int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx \u0026lt; N) { atomicAdd(out, arr[idx]); } } This kernel achieves 2.47 GB/s effective bandwidth.\nKernel 1: Vector Loads #We will start with a naive optimization. We let each thread compute a portion of the array sum in parallel, and accumulate the partial sums to the output. Since our GPU supports 128-byte load/stores, we will use the float4 vector data type.\nVector Loads __global__ void reduce_sum_kernel1(float *out, const float4 *arr, int N) { int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx \u0026lt; N) { float4 val = arr[idx]; atomicAdd(out, val.x + val.y + val.z + val.w); } } Technicaly, this kernel could even saturate the memory bus using \\(k\\)-wide vector loads and \\(k\\) sums per thread. We achieve 9.9 GB/s effective bandwidth with \\(k=4\\): a \\(4\\times\\) throughput.\nLet\u0026rsquo;s not be fooled here. This is actually not a parallel algorithm.\nComplexity analysis:\nWork complexity \\(W\\) of this kernel is \\(O(N)\\) - all elements of the array are accessed and summed once. This is the minimum work required even for a serial algorithm. Therefore, this kernel is work efficient.\nStep complexity \\(S\\) for this kernel is \\(O(N/4) \\approx O(N)\\) - as all \\(N/4\\) threads must wait for the atomicAdd to complete. This kernel is not step efficient.\nParallelism ratio \\(\\frac{W}{S} = \\frac{O(N)}{O(N)} = 1\\). This means the kernel does not scale with processors. In other words: even if we had infinite processors with infinite memory bandwidth, this kernel behaves serially under atomicAdd operations.\nAn algorithm is parallelizable if \\(\\frac{W}{S} \u0026gt; 1\\). Can we compute sum in less than \\(N\\) steps?\nKernel 2: Tree Reduction #The problem with the previous kernel is that it was not step efficient. It takes \\(O(N)\\) steps to sum over the array. Using a binary tree reduction, we can sum the array in \\(\\log N\\) steps. Note that we still do \\(N-1\\) adds and \\(N\\) reads. Therefore this approach remains work efficient.\nTree Reduction In CUDA, threads are grouped as \u0026ldquo;blocks\u0026rdquo;. We will divide the array into blocks of certain size, which is a tunable parameter. Each block will sum its elements in parallel, and then the partial sums will be accumulated using atomics. For simplicity, I will depict the reduction process for a block_size of 8.\nBlocks We load subarrays of size block_size into shared memory. We will use this buffer to store the partial sums.\n// a buffer that can hold 8 floats extern __shared__ float buffer[]; /** * blockDim.x = 8, number of threads per block * blockIdx.x = 0 or 1, id of the block * threadIdx.x = 0...7, id of the thread within the block */ int idx = blockIdx.x * blockDim.x + threadIdx.x; // thread idx within the block, 0...7 int tid = threadIdx.x; // Load a tile buffer[tid] = arr[idx]; // Don\u0026#39;t proceed until all threads have loaded their values __syncthreads(); Divergent Threads We perform the reduction once the buffer is populated. GPU invokes all blocks in parallel. Each block performs a tree reduction on the buffer. The process is as follows:\nAt the first step, consecutive elements are summed by tid=[0, 2, 4, 6]. At the second step, every second element gets summed by every second thread tid=[0, 4]. This process continues until the final sum is stored in buffer[0]. After \\(\\log N\\) reductions, each block performs an atomicAdd to the output. Note that this kernel always performs as many atomicAdd operations as there are number of blocks executing in parallel. Unlike our vector-load kernel, the number of atomic operations here is not dependent on the array size.\n__global__ void reduce_sum_kernel1(float *out, const float *arr, int N) { extern __shared__ float buffer[]; int idx = blockIdx.x * blockDim.x + threadIdx.x; int tid = threadIdx.x; if (idx \u0026lt; N) { buffer[tid] = arr[idx]; __syncthreads(); for (int s = 1; s \u0026lt; blockDim.x; s *= 2) { if (tid % (2 * s) == 0) { buffer[tid] += buffer[tid + s]; } __syncthreads(); } if (tid == 0) { atomicAdd(out, buffer[0]); } } } This kernel achieves 223 GB/s effective bandwidth, a \\(23\\times\\) improvement.\nComplexity analysis:\nWork complexity \\(W\\) of this kernel is \\(O(N)\\). Therefore, this kernel is work efficient. Step complexity \\(S\\) for this kernel is \\(O(\\log N)\\). Therefore, this kernel is step efficient. Parallelism ratio \\(\\frac{W}{S} = \\frac{O(N)}{O(\\log N)} = O(N/\\log N)\\) is greater than 1. This is our first truly parallel algorithm. Kernel 3: Non-divergent Threads #Now that we have an algorithm that is both work and step efficient, all further improvements will be a result of specializing memory and computation patterns for the actual hardware.\nThe previous kernel suffers from two issues:\nWarp divergence: Odd numbered threads remain unused. On CUDA devices, threads within a block are organized in groups of 32, called warps. When threads of a warp \u0026ldquo;branch\u0026rdquo;, the warp is said to be divergent. Compiler serializes these branches.\nModulo arithmetic: At each step, half the active threads enter the if block, perform expensive arithmetic while not participating in the computation.\n// Only evenly strided threads are active, plus % is wasteful. if (tid % (2 * s) == 0) { buffer[tid] += buffer[tid + s]; } We eliminate % operation and compute an index such that only consecutive threads in a block are active. The inner loop now becomes:\nint index = 2 * s * tid; if (index \u0026lt; blockDim.x) { buffer[index] += buffer[index + s]; } Non-divergent Threads With this change, our kernel achieves 317 GB/s effective bandwidth, a 42% improvement over the previous kernel.\nBank Conflicts4 #On CUDA devices, shared memory is divided into groups of 32 banks, assuming each index holds 4-byte wide memory (called a word). Bank ID relates to the index of memory being accessed as follows:\n$$ \\text{bank} = \\text{index } % \\text{ 32} $$\nIf different threads access different banks in parallel, shared memory can serve all those requests with no penalty. However, if two threads access indices from the same bank at the same time, the memory controller serializes these requests. These are called bank conflicts. Below is an example of a two-way bank conflict when different threads wrap around the same bank index: buffer[threadIdx.x * 2]\nTwo-way Bank Conflict For instance, with our non-divergent threads kernel:\nAt stride s=1, tid=[0,16] access index=[0,32] and index[1,33] which belong to the same bank. At stride s=2, tid=[0,8] access index=[0,32] and index[2,34] which belong to the same bank. and so on. In summary, when different threads start accessing addresses that wrap around the bank index (due to modulo 32 behavior), it may lead to 2-way, 4-way, or even higher-degree conflicts. We will see how to eliminate bank conflicts in the next kernel.\nKernel 4: Sequential Addressing #There is one neat trick up CUDA\u0026rsquo;s sleeves. It is not a bank conflict if the same thread accesses multiple addresses within the same bank. Looking at the conflict example, we want tid=0 to access index=[0,32], tid=1 to access index=[1,33] and so on. To do so, we invert the stride calculation.\nSequential Addressing // halve stride on each step for (int s = blockDim.x / 2; s \u0026gt; 0; s /= 2) { // bounds checking if (tid \u0026lt; s) { buffer[tid] += buffer[tid + s]; } __syncthreads(); } This kernel achieves 335 GB/s effective bandwidth. While this change might not seem as impressive in terms of speedup, sequential addressing and elimination of bank conflicts is the foundation for our next banger.\nKernel 5: Reduce on First Load #Half the threads in each block do not perform any computation during the first reduction step. We can halve the total blocks and let each thread perform first level of reduction while loading elements in the buffer.\nbuffer[tid] = arr[idx] + arr[idx + blockDim.x]; No Idle Threads This kernel achieves 618 GB/s effective bandwidth, over 1.86 \\(\\times\\) faster.\nKernel 6: Warp Unrolling #Sequential addressing has a key feature: threads that finish execution become idle for the rest of the program. When number of active threads become less than 32, all these threads are part of the same \u0026ldquo;warp\u0026rdquo; in their respective blocks. Threads that are part of the same warp do not need thread synchronization calls or bounds checking. This reduces instruction overhead.\nBy unrolling the reduction for last 32 elements, we eliminate useless work in all warps of all blocks.\n// Reduce synchronously across blocks until we have 32 threads left. for (int s = blockDim.x / 2; s \u0026gt; 32; s /= 2) { if (tid \u0026lt; s) { buffer[tid] += buffer[tid + s]; } __syncthreads(); } // Unroll and reduce for the last 32 threads. if (tid \u0026lt; 32) { warpReduce(buffer, tid); } We define a device function to unroll the warp and perform reductions.\n__device__ void warpReduce(volatile float *buffer, int tid) { buffer[tid] += buffer[tid + 32]; buffer[tid] += buffer[tid + 16]; buffer[tid] += buffer[tid + 8]; buffer[tid] += buffer[tid + 4]; buffer[tid] += buffer[tid + 2]; buffer[tid] += buffer[tid + 1]; } We are now at 859 GB/s effective bandwidth, within ~2% of jax.numpy.sum.\nWho doesn\u0026rsquo;t like speed?\nAssociativity [(a + b) + c = a + (b + c)] is a trait of highly parallel algorithms: since grouping of operations does not affect the result, algorithms can compute partial results independently.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOptimizing Parallel Reduction in CUDA, M Harris\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA Ampere Architecture Datasheet\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis repo on demonstrating hardware effects on GPU is useful.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"19 February 2025","permalink":"https://masterskepticista.github.io/posts/reduce-sum/","section":"Posts","summary":"A step-by-step guide on turning simple math into a flex.","title":"Embarrassingly Parallel Reduction in CUDA"},{"content":" \u0026ldquo;Mechazilla has caught the booster.\u0026rdquo; Catching a 30-storey, 500-ton cylinder mid-air with \u0026lsquo;chopsticks\u0026rsquo; is a technological marvel. But as I watched SpaceX’s Starship Flight 5 recap, it was the music that captured my imagination. It is often the part most people overlook. To me, music is more than background—it is a narrative, a source of inspiration. I set out to find the original tracks behind these historic moments.\nStarship | Fifth Flight Test #This was SpaceX team\u0026rsquo;s first (successful) attempt at catching the Super Heavy booster, which is why this 2-minute montage was more special than all previous ones.\nMain track: Ancient Streets, The Last City Pieces of the main track are scattered all over the video. The sonic boom debut at 1:55 is both visually and sonically prominent. It is such a bold way of signalling the booster\u0026rsquo;s arrival.\nStarship | Sixth Flight Test #The goal of this flight test was to test re-ignition of the Starship engine in space. Sad that we didn\u0026rsquo;t get to see a chopstick booster catch this time.\nMain track: Zero Sum, Factor One Outro (starts at 1:15 in the video): Variables, Factor One. 10 seconds in, and we know this flight test is focused on the pace of iteration. The overlap of the engine re-light and the beat of the background score starting 0:55 and beyond couldn\u0026rsquo;t be better\u0026hellip;\nThe outro has a crescendo, emphasizing on turnaround to Flight 7. I think this outro is the best of all Starship Flight Test recaps so far (see those sonic booms mingling in the white smoke during the static fire).\nStarship | Seventh Flight Test #This did not go as planned. The control to Starship was lost in the ascent burn due to a propellant leak.\nMain track: Mission Ready, Factor One It is a tough job to keep the admirers hooked when you have had 6 launch recaps over the course of a year. The first time I watched this recap I was not impressed - the background score felt muted and secondary. This was the first successful booster catch after Flight 5 and the outro crescendo from Flight 6 raised my expectations.\nWhat was the catch then? I think the visuals in this recap do the job of hooking you in. Switch to 4K60 and skip to 0:38. Now watch it a few times, and feel the tempo starting to grow on you.\nThere is a sonic boom debut at 1:57, which is where you may find the underscore in the background very appealing. To me, the notes throughout this track beam a light of courage, hope, and consistency.\nOh, and then more sonic booms at 2:46.\n","date":"21 December 2024","permalink":"https://masterskepticista.github.io/posts/starship-score/","section":"Posts","summary":"Hunting down music from SpaceX\u0026rsquo;s Starship Flight Test recaps.","title":"'Notes' of a Launch"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/music/","section":"Tags","summary":"","title":"Music"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/spacex/","section":"Tags","summary":"","title":"Spacex"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/dimensionality/","section":"Tags","summary":"","title":"Dimensionality"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/geometry/","section":"Tags","summary":"","title":"Geometry"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/neural-networks/","section":"Tags","summary":"","title":"Neural Networks"},{"content":" Grant recently published a video on how certain geometry puzzles become trivial to solve, when one additional dimension is available for the kind of geometry at hand. Math provides the necessary abstraction to think (and compute) in a dimensionality that far exceeds human imagination of 3 dimensions.\nModern foundation models use more than 10,000 feature vector dimensions. As of writing, few tools exist to understand what exactly happens in each of these dimensions. The goal of this post is not to explain what additional dimensions do, but to reason about how dimensionality affects learning, and why more dimensions typically yield better models. If you have read Chris Olah\u0026rsquo;s decade-old post on manifold hypothesis, first half of this post should be a cakewalk.\nData as a Manifold #Classification is a common deep learning task. Even foundation LLMs are trained on next-token prediction, which is a classification task. We can view classification as a way of transforming the data manifold into representations that can be sliced into individual classes.\nThe manifold hypothesis is that natural data forms lower-dimensional manifolds in its embedding space. [\u0026hellip;] If you believe this, then the task of a classification algorithm is fundamentally to separate a bunch of tangled manifolds.\nNeural networks (NNs) are efficient at extracting representations from data by augmenting it into certain size of representations. Dimensionality of the representations that NNs extract from natural data depends on what is expected of them. For example, sentiment classification models may require a smaller data manifold, than say, summarizing it, if both models are trained from scratch until convergence.\nA Number Line #What better place to start an article like this with 1D space? Consider a simple binary classification problem. We have two classes, 0 and 1, represented by three blobs on a number line. The inner blue blob is class 1, and the outer red blobs are class 0.\n1D Scatter We know via the universal approximation theorem, that a neural network with at least one hidden layer (of arbitrary dimensionality) can approximate any function.\nclass Brain(nn.Module): \u0026#34;\u0026#34;\u0026#34;Brain with a single hidden layer.\u0026#34;\u0026#34;\u0026#34; dimensions: int @nn.compact def __call__(self, inputs): out = {} x = nn.Dense(self.dimensions, kernel_init=nn.initializers.xavier_uniform())(inputs) x = out[\u0026#34;activation\u0026#34;] = nn.tanh(x) x = nn.Dense(1, name=\u0026#34;classifier\u0026#34;)(x) return x, out On a number line, the only way to classify these two classes is to find a threshold value. This threshold value is the decision boundary.\nWe need some boilerplate to train this model. This is a vanilla SGD-based training loop with no fancy regularization or momentum.\ndef fit(brain: nn.Module, params, x: jnp.ndarray, y: jnp.ndarray): \u0026#34;\u0026#34;\u0026#34;Fit a classification model to the given data.\u0026#34;\u0026#34;\u0026#34; @jax.jit def train_step(params, x, y, opt_state): def loss_fn(params): logits, out = brain.apply({\u0026#34;params\u0026#34;: params}, x) logits = jnp.squeeze(logits) loss = optax.sigmoid_binary_cross_entropy(logits, y).mean() acc = jnp.mean((logits \u0026gt; 0.5) == y) metrics = {\u0026#34;loss\u0026#34;: loss, \u0026#34;acc\u0026#34;: acc} return loss, (metrics, out[\u0026#34;activation\u0026#34;]) (_, (metrics, acts)), grads = jax.value_and_grad(loss_fn, has_aux=True)(params) updates, opt_state = tx.update(grads, opt_state, params) params = optax.apply_updates(params, updates) return params, opt_state, metrics, acts # Initialize optimizer total_steps = 4_000 tx = optax.sgd(learning_rate=0.01) opt_state = tx.init(params) # Training loop step = 1 for step in range(1, total_steps + 1): params, opt_state, metrics, acts = train_step(params, x, y, opt_state) if step % 200 == 0 or step == 1: print(f\u0026#39;Step {step}, Loss: {metrics[\u0026#34;loss\u0026#34;]:.4f}, Acc: {metrics[\u0026#34;acc\u0026#34;]:.4f}\u0026#39;) return params Classification in 1D #To match the 1D world of a number line, we will initialize our model with one dimension. The decision boundary of this neuron will be a point on the number line.\nbrain = Brain(dimensions=1) rng, rng_init = jax.random.split(rng) params = brain.init(rng_init, jnp.zeros_like(x))[\u0026#34;params\u0026#34;] params = fit(brain, params, x, y) Observe how the activation manifold evolves over time. Since we cannot go anywhere but left or right, the best-accuracy scenario is to push the blue blob as far away from the red blobs as possible, leaving half of red blobs wrongly classified. This gives exactly 75% accuracy.\nAdding a virtual dimension #But what if we could think in 2D? Ah, then the problem becomes trivial - we achieve 100% accuracy. Activation space is now a 2D plane, and the decision boundary is a line. Notice how blue cluster is stretched out orthogonally to the red blobs.\n1D to 2D Manifold Concentric Circles #Let\u0026rsquo;s redo this exercise, but starting in 2D space. Consider a binary classification problem where the inner blue blob is class 1, and the red ring is class 0.\nx, y = datasets.make_circles(n_samples=2048, noise=0.15, factor=0.1) x = (x - x.mean(axis=0)) / x.std(axis=0) fig = px.scatter(x=x[:, 0], y=x[:, 1], color=y, color_continuous_scale=\u0026#39;RdBu\u0026#39;, opacity=0.5) fig.update_layout(width=600, height=600, coloraxis_showscale=False, template=\u0026#34;plotly_white\u0026#34;) fig.show() 2D Circles Classification in 2D #To match the 2D world, we will initialize our model with two dimensions. Visualizing the activation manifold of this 2D model shows the learnt decision boundary. Since activation manifold is 2D, our decision boundary will be a line.\nbrain = Brain(dimensions=2) rng, rng_init = jax.random.split(rng) params = brain.init(rng_init, jnp.zeros_like(x))[\u0026#34;params\u0026#34;] params = fit(brain, params, x, y) 2D Manifold Evolution This behavior is similar to the case above where 1D line was expanded to 2D. The entire blue cluster is cornered with red ring stretched out along perpendicular directions. We achieve \\(\\sim\\) 85% accuracy.\nBut we don\u0026rsquo;t achieve 100% accuracy on this problem. No line segment can partition these two clusters in 2D space. The outer ring fully covers the inner one. It is mathematically impossible to do so, without using additional dimensions of space. The fact that despite topological limitations, this 2D model crossed \\(\\sim\\) 85% on this dataset tells us how far even a 2D model can twist the data manifold for classification.\nAdding a virtual dimension #Let\u0026rsquo;s see what happens when our model is given one extra dimension than the data manifold resides in. Our activation manifold will now be 3D.\nbrain = Brain(dimensions=3) rng, rng_init = jax.random.split(rng) params = brain.init(rng_init, jnp.zeros_like(x))[\u0026#34;params\u0026#34;] params = fit(brain, params, x, y) The decision boundary is now a plane, which can separate the two classes with \\(\\sim\\) 100% accuracy. This is because the model now stretches the center cluster out across z-axis, and slices a plane orthogonal to it.\n","date":"11 November 2024","permalink":"https://masterskepticista.github.io/posts/dimensionality/","section":"Posts","summary":"\u0026rsquo;nuff said about the curse of dimensionality","title":"The Blessing of Dimensionality"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/breadth-first-search/","section":"Tags","summary":"","title":"Breadth-First Search"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/depth-first-search/","section":"Tags","summary":"","title":"Depth-First Search"},{"content":" While diving into autograd internals, I had to pause at Topological sort — an important step in backpropagation. Studying it alone felt like buying a single sock. Since I would have to brush up on Depth-First Search anyway (Topological sort\u0026rsquo;s older sibling), I decided to also toss Breadth-First Search into the cart. After all, who goes shopping for algorithms and leaves with just one?\nToy Problem #We will pick a problem of finding a path from the origin (top-left) to the sink (bottom-right) of a randomly generated maze. Our definition of a maze, here, is a mesh of cells with distinct paths that can (or cannot) be traversed. It can be encoded as a graph. There are several ways to encode a graph. We will encode ours as an adjacency matrix, with 1 representing a wall and 0 representing a pathway.\nTip: Execute these cells in a notebook for an auto-updating plot. def draw(maze: np.ndarray): \u0026#34;\u0026#34;\u0026#34;Auto-updating plot.\u0026#34;\u0026#34;\u0026#34; display.clear_output(wait=True) plt.figure(figsize=(4, 4)) plt.axis(\u0026#34;off\u0026#34;) plt.imshow(maze, cmap=\u0026#34;plasma_r\u0026#34;) plt.show() I will use a simple recusive backtracking algorithm to generate mazes. Here is a goldmine of other algorithms (with beautiful animations) for the curious mind.\ndef generate_maze(n: int) -\u0026gt; np.ndarray: \u0026#34;\u0026#34;\u0026#34;Generates a random N*N maze using recursive backtracking.\u0026#34;\u0026#34;\u0026#34; # Make `n` odd. Why? n -= (n % 2) n += 1 maze = np.ones((n, n), dtype=np.int32) # Opening at the top and bottom. We choose these points # because we can guarantee that an odd-maze will # not have doubly-thick walls. maze[0][1] = maze[-1][-2] = 0 # Direction vectors. Moving by 2 units ensures # that we skip over the walls and move from one # potential passage to next. directions = [(0, 2), (0, -2), (2, 0), (-2, 0)] # Choose a random odd coordinate. start = (random.randrange(1, n, 2), random.randrange(1, n, 2)) maze[start] = 0 stack = [start] while stack: cy, cx = stack[-1] # Get neighbors in a random order. random.shuffle(directions) found_unvisited_neighbor = False for dx, dy in directions: nx, ny = cx + dx, cy + dy # Check if the candidate cell is not out of bounds and is a wall. if (0 \u0026lt;= nx \u0026lt; n and 0 \u0026lt;= ny \u0026lt; n and maze[ny][nx] == 1): # Pave thru the wall. maze[ny][nx] = 0 maze[cy + dy // 2][cx + dx // 2] = 0 # Append new location for paving stack.append((ny, nx)) found_unvisited_neighbor = True break # Backtrack if all neighbors have been visited. if not found_unvisited_neighbor: stack.pop() return maze random.seed(42) maze = generate_maze(50) draw(maze) Our goal is to search for a path from the origin (top-left) to sink (bottom-right). During our maze traversal, we are free to explore neighbors one step away.\ndef get_neighbors(y, x): return [ (y + dy, x + dx) for dy, dx in [(1, 0), (-1, 0), (0, 1), (0, -1)] ] Depth-First Search #A key feature of this algorithm is that it exhaustively searches through all possible sub-vertices connected to a given vertex, before backtracking and moving to a different vertex at the same level. It is oblivious to how close it might be to its goal. Other words, even if it is very close to a solution, and then yeets off to a random sub-vertex, it will not return until it has exhaustively searched the dead end.\ndef dfs(graph: np.ndarray, start: tuple[int, int] = (0, 1), end: tuple[int, int] = (-1, -2), visualize: bool = False) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Checks if a path exists between `start` and `end`.\u0026#34;\u0026#34;\u0026#34; visited = np.zeros_like(graph).astype(np.bool_) candidates = [start] visited[start] = True found = False while candidates: if visualize: draw(np.where(visited, 0.5, maze)) # Pick the most recent candidate, i.e. LIFO cy, cx = candidates.pop(-1) if visited[end]: found = True break for ny, nx in get_neighbors(cy, cx): if ( # within bounds? 0 \u0026lt;= ny \u0026lt; graph.shape[0] and 0 \u0026lt;= nx \u0026lt; graph.shape[1] and not visited[ny][nx] and # not a wall? graph[ny][nx] != 1 ): candidates.append((ny, nx)) visited[ny][nx] = True return found dfs(maze, visualize=True) DFS search time is proportional to the number of vertices in our adjacency matrix - \\(O(N^2)\\). The space required to store intermediate states is proportional to the number of vertices (since visited array and candidates stack are the only auxiliary objects in our function), therefore \\(O(N^2)\\).\nBreadth-First Search #A natural modification to DFS could be made on candidate selection. In fact, graph search algorithms like A* and Dijkstra are merely intelligent ways of \u0026lsquo;choosing where to search\u0026rsquo;. For BFS, instead of going down the rabbit-hole on a single vertex, what if we first explore all vertices available at a given level?\ndef bfs(graph: np.ndarray, start: tuple[int, int] = (0, 1), end: tuple[int, int] = (-1, -2), visualize: bool = False) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Checks if a path exists between `start` and `end`.\u0026#34;\u0026#34;\u0026#34; visited = np.zeros_like(graph).astype(np.bool_) candidates = [start] visited[start] = True found = False while candidates: if visualize: draw(np.where(visited, 0.5, maze)) # Pick candidates in the order they were added, i.e. FIFO. # Observe that this is the *only* change from DFS! cy, cx = candidates.pop(0) if visited[end]: found = True break for ny, nx in get_neighbors(cy, cx): if ( # within bounds? 0 \u0026lt;= ny \u0026lt; graph.shape[0] and 0 \u0026lt;= nx \u0026lt; graph.shape[1] and not visited[ny][nx] and # not a wall? graph[ny][nx] != 1 ): candidates.append((ny, nx)) visited[ny][nx] = True return found bfs(maze, visualize=True) Breadth-First Search BFS has the same time and space complexity as DFS in this example - \\(O(N^2)\\). It also feels parallel. It is because it switches between candidates very quickly - exploring horizontally across each level before exhausting and going to the next level in depth.\nWe can do a microbenchmark to see that neither of DFS or BFS is relatively faster. This is expected for large, uniformly random mazes. Real world graphs typically carry structure bias towards depth (or width). Hence, your choice of DFS/BFS should be informed by the structure of the graph. # takes ~1 min %timeit -n 5 -r 10 dfs(generate_maze(500)) %timeit -n 5 -r 10 bfs(generate_maze(500)) 749 ms ± 58.2 ms per loop (mean ± std. dev. of 10 runs, 5 loops each) 778 ms ± 58.8 ms per loop (mean ± std. dev. of 10 runs, 5 loops each) Topological Sort #Now to the intended goalpost - topological sort. It helps to understand why we need it in the first place. Neural networks are directed acyclic graphs that take a set of input tensor(s), and through multiple operations, return a set of output tensor(s), on which we compute loss, a proxy for how well the network is behaving.\nThe training objective of neural networks is to minimize this loss by moving along a direction (a.k.a derivative) and [back]propagating this feedback from the loss node to the inputs. Each node in a neural network \u0026lsquo;knows\u0026rsquo; how to compute its own derivative only when a gradient signal from the nodes succeeding it is available.\nBut in a graph with (easily) hundreds of nodes and edges, the autograd engine cannot do a big bang calculation of everything. It must know the exact order of reverse traversal - both for correctness and efficiency of compute.\nWe will emulate a neural network by generating a web of nodes that are randomly connected to each other between layers. We control this randomness (ten-dollar word is sparsity) using a parameter. Note that if sparsity is too high (say above 0.5), you may end up with unconnected layers. It won\u0026rsquo;t impact our intent though. I even recommend you experiment with different values to understand how toposort identifies viable orderings.\nBear with me on this boilerplate code to visualize our graph, before we implement toposort(...).\nimport itertools import networkx as nx def generate_mlp( layers: list[int], sparsity: float = 0., seed: int = 42 ) -\u0026gt; tuple[nx.DiGraph, dict]: \u0026#34;\u0026#34;\u0026#34;Generate a layer-wise sparsely connected MLP. Args: layers: A list of integers denoting nodes in each layer. First and last are considered input and output nodes respectively. sparsity: A float between [0, 1] representing how sparse the graph should be. 0 means no sparsity, i.e. a fully-connected MLP. seed: For reproducibility. Returns: A tuple of nx.DiGraph and options to plot it pretty. \u0026#34;\u0026#34;\u0026#34; random.seed(seed) num_layers = len(layers) # Assign labels to each node. input_nodes = [f\u0026#34;$x_{i}$\u0026#34; for i in range(layers[0])] hidden_layers = [ [f\u0026#34;$h^{lyr}_{i}$\u0026#34; for i in range(layers[lyr])] for lyr in range(1, num_layers - 1) ] output_nodes = [f\u0026#34;y{i}\u0026#34; for i in range(layers[-1])] layers = [input_nodes, *hidden_layers, output_nodes] # Assign nodes. G = nx.DiGraph() for lyr, nodes in enumerate(layers): G.add_nodes_from(nodes, layer=lyr) # Assign random edges. for left_layer, right_layer in zip(layers[:-1], layers[1:]): connections = list(itertools.product(left_layer, right_layer)) keep_p = 1 - sparsity G.add_edges_from(connections[:int(keep_p * len(connections))]) # Pretty plot. pos = nx.multipartite_layout(G, subset_key=\u0026#34;layer\u0026#34;) colors = [\u0026#34;gold\u0026#34;] + [\u0026#34;violet\u0026#34;] * (num_layers - 2) + [\u0026#34;limegreen\u0026#34;] options = { \u0026#34;node_color\u0026#34;: [colors[data[\u0026#34;layer\u0026#34;]] for node, data in G.nodes(data=True)], \u0026#34;with_labels\u0026#34;: True, \u0026#34;node_size\u0026#34;: 1000, \u0026#34;pos\u0026#34;: pos, } return G, options G, options = generate_mlp([3, 4, 6, 4, 6, 1], sparsity=0.5) nx.draw(G, **options) plt.show() The only important observation is that not all nodes may influence the terminal loss node (indexed as \\(y_i\\)) in this example. Our sorting algorithm should identify this.\nIn other words, for a root node loss, the autograd engine needs a list of all dependencies to loss all the way to input tensor(s), in the order they impact it. This is like searching along the depth from the loss node (did it click in your mind?). We will use toposort to find these orderings from a given root node to the stopping points on the graph (i.e. nodes with no more ancestors) via depth-first search.\ndef toposort(G: nx.DiGraph, root: str): \u0026#34;\u0026#34;\u0026#34;Returns a topological ordering from `root` on the graph `G`.\u0026#34;\u0026#34;\u0026#34; visited = set() order = [] def _dfs(node): if node not in visited: visited.add(node) # If the node has ancestors, DFS along each. for parent in G.predecessors(node): _dfs(parent) # Once all ancestors have been evaluated, # add the node to the topological order. order.append(node) # Start searching for stopping points from the root. _dfs(root) # Because we start from the root node. order.reverse() return order order = toposort(G, \u0026#39;y0\u0026#39;) order ['y0', '$h^4_2$', '$h^4_1$', '$h^4_0$', '$h^3_1$', '$h^3_0$', '$h^2_2$', '$h^2_1$', '$h^2_0$', '$h^1_1$', '$h^1_0$', '$x_1$', '$x_0$'] Modulo the ugly \\(\\LaTeX\\) formatting, We can see that toposort returns an ordering from output node(s) to input node(s) whenever there is a path connecting them.\nLet\u0026rsquo;s draw these backward edges.\ndef draw_backward_traversal(G: nx.DiGraph, order: list[str]): \u0026#34;\u0026#34;\u0026#34;Plots the reverse traversal of nodes given in `order` on the graph `G`.\u0026#34;\u0026#34;\u0026#34; edges = [] for node in order: # Point incident edges backwards incident_edges = G.in_edges(node) incident_edges = [e[::-1] for e in incident_edges] edges.extend(incident_edges) nx.draw(G, edgelist=edges, edge_color=\u0026#34;red\u0026#34;, **options) plt.show() draw_backward_traversal(G, order) See? The topological ordering automatically ignores nodes (even inputs) that do not influence the output.\nThis is an unconventional example of \u0026lsquo;sparsely-connected\u0026rsquo; MLPs. In practice, MLPs are trained fully-connected. Some may find it easier to imagine these graph nodes are arbitrary functions themselves, instead of MLPs. We knew it all along, didn\u0026rsquo;t we? #This exercise taught me that algorithms of this class are actually straightforward to understand if you have a view of the entire graph and you know the objective. The challenge lies in expressing your computation in a way computers can understand. Specifically:\nHow can you identify a sub-problem that can be recursed/iterated over. How and when to recurse/iterate over that subproblem, and How to book-keep states efficiently. We take for granted how much our brain does this in a diffused manner. Luckily, with frontier language models getting better, \u0026rsquo;talking\u0026rsquo; with computers will get easier.\nI hoped to cover Dijkstra and A* as well, given how they add a sprinkle of intelligence (read heuristics) to solve certain classes of graph problems faster. That will be a different article for now.\n","date":"27 August 2024","permalink":"https://masterskepticista.github.io/posts/graph-search/","section":"Posts","summary":"Time for an algorithm shopping spree.","title":"Graph Search"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/graph-search/","section":"Tags","summary":"","title":"Graph Search"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/topological-sort/","section":"Tags","summary":"","title":"Topological Sort"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/detr/","section":"Tags","summary":"","title":"Detr"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/jax/","section":"Tags","summary":"","title":"Jax"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/object-detection/","section":"Tags","summary":"","title":"Object-Detection"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/pytorch/","section":"Tags","summary":"","title":"Pytorch"},{"content":" PyTorch is far from being concluded slow. But it is always a fun (and worthwhile) exercise to flex how fast you can really go with compilers if you can lay out a computation in the right way.\nThis is my work log of building a Detection Transformer (DETR) training pipeline in JAX. I find this object detection architecture special for many reasons:\nIt predicts bounding boxes and class labels directly, instead of generating a gazillion region-proposals and relying on esoteric post-processing techniques. It is end-to-end differentiable and parallelizable. It fits the \u0026lsquo;spirit of deep learning\u0026rsquo; and borrows wisdom from Rich Sutton\u0026rsquo;s Bitter Lesson of AI research. DETR is one of the well written papers out there, I recommend going through it once.\nHowever, DETR is slow to train. While there have been successors to DETR that improve algorithmic convergence rates, like Deformable-DETR, or Conditional-DETR, none of these implementations focus on running \u0026rsquo;efficiently\u0026rsquo; on the GPU. There is a great deal of efficiency to be had here, which was the objective of this project. I will walk through the techniques that helped me provide up to \\(30\\%\\) higher GPU utilization against a best-effort optimized PyTorch implementation of DETR.\nThe Bottleneck # DETR Architecture DETR has three main components: a convolutional backbone (typically a ResNet), a stack of encoder-decoder transformer blocks, and a bipartite matcher. Of the three, bipartite matching (hungarian) algorithm runs on the CPU. In fact, the original DETR implementation calls scipy.optimize.linear_sum_assignment sequentially, for each input-target pair. This leaves the GPU idle. Part of the gains we will see later, are by reducing this idle time.\nIdle GPU is wasted GPU.\nBaseline #To improve \u0026lsquo;something\u0026rsquo;, we must \u0026lsquo;measure\u0026rsquo; that something. Our bench this time is an 8-A6000 cluster. I made a couple of changes to ensure PyTorch version was \u0026lsquo;as fast as possible\u0026rsquo;. Here is a summary of digressions:\nUse Flash Attention in F.scaled_dot_product_attention. Enable use of tensor cores by setting torch.set_float32_matmul_precision(...) to \u0026ldquo;medium\u0026rdquo; (\u0026ldquo;high\u0026rdquo; works just as good). Wrap forward pass using torch.autocast to bfloat16. With these changes, it took 3 days (2.1 steps/s) to train a 300-epoch baseline on our cluster. I will skip the napkin math, but this is already faster than authors\u0026rsquo; numbers when normalized for per GPU FLOP throughput - notably from use of the new flash attention kernel that Ampere GPUs support.\nI tried torch.compile with various options on sub-parts of the model/training step. It either ended up giving the same throughput, or failed to compile. So \u0026lsquo;it is what it is\u0026rsquo;. Refactor #I decided to implement DETR in JAX. You can think of JAX as a front-end language to write XLA optimized programs. XLA is an open-source Machine Learning compiler that optimizes Linear Algebra operations. XLA generally outperforms the superset of all PyTorch optimizations when done right, by a good margin. One downside of working with XLA/JAX is that it is harder to debug jit compiled programs. PyTorch, on the other hand, dispatches CUDA kernels eagerly (except when wrapped with torch.compile), which makes it easiest to debug and work with. But when you consider the cost of few compile minutes over how long production training runs like these typically are, it is worth the tradeoff.\nLuckily a dusty re-implementation of DETR in JAX made for a good head-start. But it did not work out-of-the-box due to deprecated JAX and Flax APIs. To get the ball rolling, I made a minimal set of changes, without any optimizations.\nScenic also provides GPU and TPU implementations of Hungarian matching. This is already significant work off-the-table.\nThis implementation takes 6.5 days to replicate the PyTorch baseline, at nearly 1 step/s. How fast can we go?\nNow, the optimizations.\n1. Disable Matching for padded objects #This is actually a bug-fix rather than an optimization. COCO dataset does not guarantee a fixed number of objects for each image. This means the bipartite matcher would have to map a fixed set of object queries (say 100) to a randomly varying number of target objects for each image, triggering an expensive retrace of the graph.\nXLA compiler can generate optimized graphs in part because memory allocation/deallocation is predictable, and constant-folding/fusion of operators is simpler when the entire computational graph layout is static. This is the price you pay for performance. You can read more here. To prevent retracing, we add \u0026lsquo;padding\u0026rsquo; objects and a boolean mask that allows us to filter dummy objects when computing loss.\n# Adding padded dimensions # input_pipeline.py#L145 padded_shapes = { \u0026#39;inputs\u0026#39;: [max_size, max_size, 3], \u0026#39;padding_mask\u0026#39;: [max_size, max_size], \u0026#39;label\u0026#39;: { \u0026#39;boxes\u0026#39;: [max_boxes, 4], \u0026#39;area\u0026#39;: [max_boxes,], \u0026#39;objects/id\u0026#39;: [max_boxes,], \u0026#39;is_crowd\u0026#39;: [max_boxes,], \u0026#39;labels\u0026#39;: [max_boxes,], \u0026#39;image/id\u0026#39;: [], \u0026#39;size\u0026#39;: [2,], \u0026#39;orig_size\u0026#39;: [2,], }, } But this still computes bipartite matching on padded objects. We can remove constants from the cost matrix as they do not affect the final matching.\n-- cost = cost * mask + (1.0 - mask) * cost_upper_bound ++ cost = cost * mask With this bug-fix, we are now 40% faster, i.e. \\(1.4\\) steps/s. It now takes 4.7 days to train the baseline.\n2. Mixed Precision MatMuls #Yes, there are no \u0026lsquo;free-lunches\u0026rsquo;, but I think we can make a strong case for the invention of bfloat16 data type. We migrate float32 matmuls to bfloat16, without any loss in final AP scores. This is what we did in the PyTorch baseline. In flax, this is the same as supplying dtype=jnp.bfloat16 on supported modules.\n# Example conversion. conv = nn.Conv(..., dtype=jnp.bfloat16) dense = nn.Dense(..., dtype=jnp.bfloat16) ... This gets us above \\(2.1\\) steps/s. We now have performance parity with PyTorch, with 3.1 days taken to train the baseline!\nHuh! We should\u0026rsquo;ve called it a day\u0026hellip; but let\u0026rsquo;s keep going.\n3. Parallel Bipartite Matching on Decoders #To achieve a high overall \\(\\text{mAP}\\) score, DETR authors propose computing loss over each decoder output. DETR uses a sequential stack of 6 decoders, each emitting bounding-box and classifier predictions for 100 object queries.\n# models/detr_base_model.py#L377 # Computing matchings for each decoder head (auxiliary predictions) # outputs = { # \u0026#34;pred_logits\u0026#34;: ndarray, # \u0026#34;pred_boxes\u0026#34;: ndarray, # \u0026#34;aux_outputs\u0026#34;: [ # {\u0026#34;pred_logits\u0026#34;: ndarray, \u0026#34;pred_boxes\u0026#34;: ndarray}, # {\u0026#34;pred_logits\u0026#34;: ndarray, \u0026#34;pred_boxes\u0026#34;: ndarray}, # ... # ] # } if matches is None: cost, n_cols = self.compute_cost_matrix(outputs, batch[\u0026#39;label\u0026#39;]) matches = self.matcher(cost, n_cols) if \u0026#39;aux_outputs\u0026#39; in outputs: matches = [matches] for aux_pred in outputs[\u0026#39;aux_outputs\u0026#39;]: cost, n_cols = self.compute_cost_matrix(aux_pred, batch[\u0026#39;label\u0026#39;]) matches.append(self.matcher(cost, n_cols)) Computing optimal matchings on these decoder outputs can actually be done in parallel using vmap.\n# models/detr_base_model.py#L377 # After vectorization if matches is None: predictions = [{ \u0026#34;pred_logits\u0026#34;: outputs[\u0026#34;pred_logits\u0026#34;], \u0026#34;pred_boxes\u0026#34;: outputs[\u0026#34;pred_boxes\u0026#34;] }] if \u0026#39;aux_outputs\u0026#39; in outputs: predictions.extend(outputs[\u0026#34;aux_outputs\u0026#34;]) def _compute_matches(predictions, targets): cost, n_cols = self.compute_cost_matrix(predictions, targets) return self.matcher(cost, n_cols) # Stack list of pytrees. predictions = jax.tree.map( lambda *args: jnp.stack(args), *predictions) # Compute matches in parallel for all outputs. matches = jax.vmap(_compute_matches, (0, None))( predictions, batch[\u0026#34;label\u0026#34;]) matches = list(matches) With this change, we are now stepping 10% faster than PyTorch, at \\(2.4\\) steps/s, i.e. 2.7 days to train.\n4. Use Flash Attention #XLA did not use flash attention kernel all along. It was added only recently through jax.nn.dot_product_attention for Ampere and later architectures. Perhaps future XLA versions might automatically recognize a dot-product attention signature during jit, without us having to explicitly call via SDPA API. But that is not the case today, so we will make-do with this custom function call.\n# models/detr.py#L261 if True: x = jax.nn.dot_product_attention( query, key, value, mask=mask, implementation=\u0026#34;cudnn\u0026#34;) else: x = attention_layers.dot_product_attention( query, key, value, mask=mask, dropout_rate=self.dropout_rate, broadcast_dropout=self.broadcast_dropout, dropout_rng=self.make_rng(\u0026#39;dropout\u0026#39;) if train else None, deterministic=not train, capture_attention_weights=False) As of writing, jax.nn.dot_product_attention does not support attention dropout. This is because JAX and cuDNN use different PRNG implementations. Speedup outweighs the regularization benefits of dropout, so we will live with it for now. For now, let us be content with the potential speedup. We are now at \\(3.0\\) steps/s, 33% faster than PyTorch, taking 2 days to train.\nSummary #Further gains are possible by replacing exact matching with an approximate matching. It may be a good reason to do so - just like how minibatch SGD is random by its very nature. It is arguably its strong suit on its nice convergence properties.\nWhy should a matching algorithm be exact, if we are spending ~0.5M steps to converge anyway? Are there gains to be had by having an \u0026lsquo;approximate\u0026rsquo; matching? Yes, and one way to go about it is using a regularized solver like Sinkhorn algorithm. But that\u0026rsquo;s for another day.\nYou can find the code for DETR with all above optimizations here. Update: It also supports Sinkhorn algorithm now!\n","date":"16 August 2024","permalink":"https://masterskepticista.github.io/posts/detr/","section":"Posts","summary":"A recipe to train Object Detection Transformers (really) fast.","title":"PyTorch: I’m Fast, JAX: You Call That Fast?"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/transformers/","section":"Tags","summary":"","title":"Transformers"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/data-parallelism/","section":"Tags","summary":"","title":"Data Parallelism"},{"content":" Big model ask big money. The limiting factor on the size of models that can be trained, can be summarized to data movement speeds - either within the GPU (HBM bandwidth) or across GPUs (collective ops bandwidth). A big part of model scaling with the number of accelerators is achieved by reducing communication overhead across them. This is why protocols like Infiniband/NVLink exist.\nBut can we get away without spending a fortune on 100G/400G NICs for training models across nodes? Turns out, under the right assumptions, we can.\nInfrastructure #We had four1 server blades each with the following spec:\nDual socket Xeon 6258R (28C/56T per socket) 512GB DDR4 Memory One RTX-3090 GPU Intel X540 Dual-port 10GbE, one of the ports was utilized for Internet via a 1GbE link. Goals #We had to consolidate these servers into a multi-node training cluster:\nWith above 90% scaling factor. With support for medium sized models (think ResNet-50/101 or ViT-S/B) up to 100M params. Using 10G Ethernet only. Any other NIC would require a new switch and a new card. The constraint: spend $0.\nSingle GPU Training: Baseline #Our focus was on data-parallel training since we could fit all our models on a single GPU (modulo tuning the batch size).\nTake ResNet50 for example. Training a ResNet50 on 90 epochs of ImageNet-1k on a single blade takes 48h in the default TensorFloat32 precision. Since Ampere+ architectures support bfloat16, we could reduce data movement within the GPU, and saturate the Tensor Cores (Tesla architecture supports float16, but that requires gradient scaling in the training loop to avoid overflows. I won\u0026rsquo;t cover that here given there exist plenty of guides online on how to use float16).\nHere is a summary of changes:\n# JAX conv = nn.Conv(..., dtype=jnp.bfloat16) dense = nn.Dense(..., dtype=jnp.bfloat16) ... # TensorFlow (keras) tf.keras.mixed_precision.set_global_policy(\u0026#39;mixed_bfloat16\u0026#39;) # Torch with torch.autocast(device_type=device_type, dtype=torch.bfloat16): ... We wrote our training pipelines in TensorFlow + JAX for two main reasons:\nFine-grained data ETL tuning with tf.data, and Higher GPU throughput via XLA. This frees us from low-level compiler optimizations. (If you are a PyTorch user, torch.compile will do this for you) Half-precision matmuls got us down to 25h while hitting the same Top-1 score of 76.3%. We clocked a per-step time \\(T_s=230\\text{ms}\\) (i.e. time taken per forward/backward pass of a batch). This was a reasonable single GPU baseline.\nAt this point we can extrapolate to our \u0026ldquo;ideal\u0026rdquo; cluster: it would train a ResNet50 just below 6.3h with 100% scaling across four nodes.\nBandwidth Calculation #Training across nodes requires taking a global average of gradients across all GPUs on each backward pass. So each GPU would have to send and receive a full set of gradient_size data at each step.\n$$ \\text{gradient\\_size} = \\frac{\\text{params\\_size}}{1e^6} \\times \\text{bytes\\_per\\_param} $$\nFor a ResNet50 with 25M parameters, gradient_size is roughly 100MB per step, per GPU. Since each GPU needs a full copy of globally averaged gradients - a naive algorithm would require the lead host to fetch and broadcast 100MB data to/from each GPU. This would create a massive bottleneck on the main host, since the communication time would grow linearly on the number of GPUs.\nLucky for us, most implementations2 of collectives today use the RingAllReduce algorithm, which amortizes the amount of transfers as number of GPUs increase, by communicating \u0026lsquo;chunked\u0026rsquo; gradients. In other words: data communicated per GPU reaches an asymptotic limit, independent of the number of GPUs in the cluster.\n$$ \\text{data\\_per\\_gpu} = 2 (N - 1) \\frac{\\text{gradient\\_size}}{N} = \\frac{3}{2} \\times 100 \\text{ MB} $$\nIf you are interested in the proof, Gibiansky has a great article explaining the RingAllReduce algorithm.\nIn complex topologies spanning thousands of GPUs, a HierarchicalAllReduce algorithm scales better. On our 4-node cluster with 10GbE bi-directional links, time spent in communication would be\n$$ T_c = \\frac{\\text{data\\_per\\_gpu}}{\\text{bandwidth}} = \\frac{150}{1.25 \\times 1024} = 0.11 \\text{s.} $$\nSo we would pay a fixed cost of 110ms each time, to synchronize gradients.\nMulti GPU Training: Baseline #Lets start with a simple baseline that connects all 4 blades through a 10G switch. We can measure the time spent in computation and communication using Tensorboard profiler.\nStep profile of ResNet50 With a total forward + backward pass time \\(T_s = 234\\text{ms}\\), we spend an additional \\(T_c = 106\\text{ms}\\) in communication (\u0026ldquo;NCCL\u0026rdquo;), in line with our estimate above. Note that we already save time prefetching batches to the GPU by overlapping it with computation (\u0026ldquo;Copy\u0026rdquo; step). With this information, we can calculate the scaling efficiency \\(\\eta\\) of our cluster.\n$$ \\eta = \\frac{T_s}{T_s + T_c} \\approx 68.8% $$\nNow, our ResNet50 takes 9.1h to train (i.e., a \\(2.74\\times\\) speedup over single GPU baseline). It is a sizeable jump, but notice that more than 1 GPU worth of our compute is spent idling.\nScaling efficiency baseline If we take a close look at the formulation of \\(\\eta\\), we only have two ways to increase scaling efficiency from here:\nReduce communication (\\(T_c\\)), and/or Defer communication (by increasing \\(T_s\\)). We will now explore each optimization in detail.\n1. Reducing Communication #Upto this point we communicate 25M float32 values at the end of each step. One way to reduce communication could be by compressing gradients (lossy or otherwise). Here are our options:\nCast gradients to bfloat16: No risk of overflow, but lossy due to high machine \\(\\epsilon\\)3. Cast gradients to float16: Risk of overflow, but lossless if renormalized. Use a more intelligent gradient compression schema (like sparsity?). We\u0026rsquo;ll stick with a simple technique that worked for us. We cast gradients to bfloat16 during communication. We did not observe any loss in accuracy.\n# JAX grads = jax.tree.map(lambda g: g.astype(jnp.bfloat16), grads) grads = lax.pmean(grads, axis_name=\u0026#34;batch\u0026#34;) # AllReduce grads = jax.tree.map(lambda g: g.astype(jnp.float32), grads) # Tensorflow (Keras) def compressed_aggregate_gradients(grads_and_vars): \u0026#34;\u0026#34;\u0026#34;An override for `tf.optimizers.Optimizer.aggregate_gradients` method to compress gradients before allreduce.\u0026#34;\u0026#34;\u0026#34; grads, vars = zip(*grads_and_vars) grads = [tf.cast(g, tf.float16) for g in grads] grads = tf.distribute.get_replica_context().all_reduce( tf.distribute.ReduceOp.SUM, grads) grads = [tf.cast(g, tf.float32) for g in grads] grads_and_vars = zip(grads, vars) return grads_and_vars optimizer.aggregate_gradients = compressed_aggregate_gradients Our scaling efficiency with halved communication time is:\n$$ \\eta = \\frac{T_s}{T_s + 0.5 \\times T_c} = \\frac{234}{234 + 53} \\approx 81.5\\% $$\nGradient compression results \u0026hellip;which is pretty neat! This brings down our training time from 9.1h to 7.7h.\n2. Deferring Communication #Gradient synchronization is required at the end of each batch, and there are only so many samples we can fit in a single forward/backward pass per batch\u0026hellip;\n\u0026hellip;or can we?\nGradient accumulation is a common technique to emulate large batch sizes on GPUs with limited memory. But this can also be seen as a way of deferring communication. If the maximum batch size supported on a forward/backward pass is 512, which was the case for us here, we could prepare a larger 1024-size batch, and sum over gradients within the GPU with two \u0026ldquo;micro\u0026rdquo; batches.\nThe only potential downside of this trick, is if a given model/optimizer does not scale with batch size. This could be the case for small datasets (but then why would you need data parallel?).\nHere is a simple implementation in JAX:\ndef accumulate_gradient(value_and_grad_fn, params: PyTree, batch: PyTree, accum_steps: int = 1) -\u0026gt; Tuple[jnp.ndarray, PyTree]: \u0026#34;\u0026#34;\u0026#34;Accumulates gradients over given steps. Args: value_and_grad_fn: Gradient function that does not return aux values. params: Parameters, passed as first argument to `value_and_grad_fn`. batch: Batch, passed as second argument to `value_and_grad_fn`. accum_steps: Number of micro batches to accumulate over. Defaults to 1, which means no gradients are accumulated. Returns: Tuple (loss, grads). \u0026#34;\u0026#34;\u0026#34; if accum_steps \u0026gt; 1: bs = next(iter(jax.tree.leaves(batch))).shape[0] assert bs % accum_steps == 0, ( f\u0026#34;Invalid accum_steps {accum_steps} for batch size `{bs}\u0026#34;) microbatch_size = bs // accum_steps logging.info(\u0026#34;Accumulating with microbatch_size %d over %d steps.\u0026#34;, microbatch_size, accum_steps) def get_microbatch(batch, i): return jax.tree.map( lambda t: jnp.reshape(t, (accum_steps, -1) + (t.shape[1:]))[i], batch) # Initialize accumulator. l, g = value_and_grad_fn(params, get_microbatch(batch, 0)) def accumulate(i, l_and_g): l, g = l_and_g l_i, g_i = value_and_grad_fn(params, get_microbatch(batch, i)) return (l + l_i, jax.tree.map(jnp.add, g, g_i)) # Average over accum_steps. loss, grads = jax.lax.fori_loop(1, accum_steps, accumulate, (l, g)) return jax.tree.map(lambda x: x / accum_steps, (loss, grads)) else: return value_and_grad_fn(params, batch) In theory, you could go all-in with many accumulation steps, such that the communication time as a fraction of total step time tends to zero - giving you an \\(\\eta \\approx 99\\%\\).\nIn our case, we used 2 accumulation steps to match the 4096 batch-size in BiT: BigTransfer paper. Plugging values back into our equation:\n$$ \\frac{2 \\times T_s}{2 \\times T_s + 0.5 \\times T_c} = \\frac{468}{468 + 53} \\approx 89.8\\% $$\nGradient accumulation results Ouch, we were SO close to hit our \\(90\\%\\) goal!\n3. Faster Communication #Ok, no scam going on here. We did not end up buying a faster NIC. Remember that our existing NIC had dual 10G ethernet ports - one of which was running on 1G for networking. We reconfigured all four servers to connect directly to the 10G switch, which in turn was connected to the Internet via a single 1G port.\nOn paper, we had 20G bandwidth to/from each node. The question was, did NCCL support multi-NIC? Absolutely it did! I will spare you the details of benchmarking, but these were the two flags we set for NCCL.\nNCCL_SOCKET_IFNAME=ens803f # Includes ens803f0 and ens803f1, 10G each. NCCL_SOCKET_NTHREADS=1 # May be different on your setup. With communication speed doubled, we crunch the numbers again:\n$$ \\eta = \\frac{2 \\times T_s}{2 \\times T_s + 0.25 \\times T_c} = \\frac{468}{468 + 27} \\approx 94.5\\% $$\nMulti-NIC communication results Result #This cluster achieved a throughput roughly 20% higher than a $16/hr V100 AWS instance. Our team saved ~$120k for close to a year of uptime.\nWe actually had an odd number of nodes. I rounded all calculations assuming 4, for it is a nice number for hardware.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNCCL Bandwidth and Throughput Calculation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nComparing bfloat16 range and precision to other 16-bit numbers\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"5 August 2024","permalink":"https://masterskepticista.github.io/posts/orion/","section":"Posts","summary":"Bag-of-tricks for multi-node training for the GPU Poor.","title":"Data Parallelism using standard Ethernet"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/multi-node/","section":"Tags","summary":"","title":"Multi-Node"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/tensorflow/","section":"Tags","summary":"","title":"Tensorflow"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/talks/","section":"slides","summary":"","title":"slides"},{"content":"Good engineering is craftsmanship; and great engineers derive a healthy dose of dopamine from \u0026lsquo;building\u0026rsquo; things.\nNo wonder Elon came up with this 5-step \u0026lsquo;algorithm\u0026rsquo;, building the hardest engineering marvels at SpaceX. There is a lot of this that applies even to building software.\nSo, the algorithm:\n1. Make your requirements less dumb. # Your requirements are definitely dumb.\nEngineers (and by extension most leaf-level nodes in large organizations) tend to take what is handed to them at face value.\nQuestion each requirement. If a requirement does not make sense, talk to the person who imposed the requirement. Departments do not give requirements, people do. No matter who gave you the requirement, they may be wrong. It is particularly dangerous if a smart person gave you the requirement - you may not feel like questioning their judgement. Everyone is wrong at least some of the time.\n2. Delete a part of the process. #Companies tend to incentivize adding items to process steps, \u0026lsquo;just in case we need it\u0026rsquo;. One should actively perform garbage collection on the massive web of steps, to maximize efficiency.\nIt is easy to confuse this with the idea of a Chesterton\u0026rsquo;s Fence, which says:\nDo not remove a fence until you know why it was put up in the first place.\nBut the intent is not to do a blanket deletion of steps. Instead, one must put an active effort to understand each part of the process, and eliminate pieces aggresively.\nIf you are not adding things back occasionally, you are not deleting enough. That is how you can be sure you are not being overly conservative. This is the part that trips everyone a lot.\n3. Simplify, or optimize. #It is every smart engineer\u0026rsquo;s tendency to optimize a thing that should not exist. Part of this mindset is traceable to our education system, which incentivizes convergent logic - the process of deducing the solution by applying finite rules. This is the mental equivalent of wearing a straitjacket.\nMost real world problems are not convergent in nature. Focus on the most impactful portion of your job, and iterate over that.\n4. Accelerate cycle time. #Anything can be sped up. Only after you have done the other 3 things, accelerate your iteration time.\nIf you are digging your grave, don\u0026rsquo;t dig it faster.\n5. Automate. #Don\u0026rsquo;t reverse these steps. You definitely don\u0026rsquo;t want the pain of deleting a part of the process, after already sinking the time and energy automating it.\n","date":"7 March 2024","permalink":"https://masterskepticista.github.io/posts/the-algorithm/","section":"Posts","summary":"Elon\u0026rsquo;s 5-step algorithm for building great technology.","title":"The Algorithm"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/tags/work/","section":"Tags","summary":"","title":"Work"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/papers/","section":"Papers","summary":"","title":"Papers"},{"content":"","date":null,"permalink":"https://masterskepticista.github.io/categories/","section":"Categories","summary":"","title":"Categories"}]