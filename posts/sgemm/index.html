<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=false><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#FFFFFF"><title>Optimizing a GEMM from first principles &#183; MasterSkepticista</title><meta name=title content="Optimizing a GEMM from first principles &#183; MasterSkepticista"><script type=text/javascript src=https://masterskepticista.github.io/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="></script><link type=text/css rel=stylesheet href=https://masterskepticista.github.io/css/main.bundle.min.59fb1220c398eb42c8833288370f303229bb823c802bed53e7dc6421f7a97a93.css integrity="sha256-WfsSIMOY60LIgzKINw8wMim7gjyAK+1T59xkIfepepM="><script defer type=text/javascript id=script-bundle src=https://masterskepticista.github.io/js/main.bundle.min.d97ddc5ab27d9f764a39a1e9c14f27b5583e3d47287e6278766921e42cb09f55.js integrity="sha256-2X3cWrJ9n3ZKOaHpwU8ntVg+PUcofmJ4dmkh5Cywn1U=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        Within 92% of Intel MKL (single-threaded).
      
    "><link rel=canonical href=https://masterskepticista.github.io/posts/sgemm/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://masterskepticista.github.io/posts/sgemm/"><meta property="og:site_name" content="MasterSkepticista"><meta property="og:title" content="Optimizing a GEMM from first principles"><meta property="og:description" content="Within 92% of Intel MKL (single-threaded)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-14T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-14T00:00:00+00:00"><meta property="article:tag" content="Sgemm"><meta property="article:tag" content="Avx512"><meta property="article:tag" content="Matmul"><meta name=twitter:card content="summary"><meta name=twitter:title content="Optimizing a GEMM from first principles"><meta name=twitter:description content="Within 92% of Intel MKL (single-threaded)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Optimizing a GEMM from first principles","headline":"Optimizing a GEMM from first principles","abstract":"Within 92% of Intel MKL (single-threaded).","inLanguage":"en","url":"https:\/\/masterskepticista.github.io\/posts\/sgemm\/","author":{"@type":"Person","name":"Karan Shah"},"copyrightYear":"2025","dateCreated":"2025-07-14T00:00:00\u002b00:00","datePublished":"2025-07-14T00:00:00\u002b00:00","dateModified":"2025-07-14T00:00:00\u002b00:00","keywords":["sgemm","avx512","matmul"],"mainEntityOfPage":"true","wordCount":"3007"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://masterskepticista.github.io/","name":"","position":1},{"@type":"ListItem","item":"https://masterskepticista.github.io/posts/","name":"Posts","position":2},{"@type":"ListItem","name":"Optimizing a Gemm From First Principles","position":3}]}</script><meta name=author content="Karan Shah"><link href=https://github.com/MasterSkepticista rel=me><link href=https://twitter.com/elevated_quark rel=me><link href="https://scholar.google.com/citations?user=vEgcRTkAAAAJ&amp;hl=en" rel=me><link href=https://stackoverflow.com/users/9230398/karan-shah rel=me><link href=https://linkedin.com/in/karan-bhavesh-shah rel=me><link type=text/css rel=stylesheet href=https://masterskepticista.github.io/lib/katex/katex.min.19095127357ed6d29fe0a63a6b000c913a89f7f1963b765dd3715e97c9852e75.css integrity="sha256-GQlRJzV+1tKf4KY6awAMkTqJ9/GWO3Zd03Fel8mFLnU="><script defer src=https://masterskepticista.github.io/lib/katex/katex.min.e8d885505949f3a5f4abdd5dd0d53696bd1371ad26ffbf4f310dcd77c8cdae89.js integrity="sha256-6NiFUFlJ86X0q91d0NU2lr0Tca0m/79PMQ3Nd8jNrok="></script><script defer src=https://masterskepticista.github.io/lib/katex/auto-render.min.bb53eb953394531aae36fdd537065c4244eb8542901a3ce914601d932675b8ac.js integrity="sha256-u1PrlTOUUxquNv3VNwZcQkTrhUKQGjzpFGAdkyZ1uKw=" onload=renderMathInElement(document.body)></script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden"><nav class="flex items-start justify-between sm:items-center"><div class="z-40 flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>MasterSkepticista</a></div><label id=menu-button for=menu-controller class="block sm:hidden"><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper class="invisible fixed inset-0 z-30 m-auto h-full w-full cursor-default overflow-auto bg-neutral-100/50 opacity-0 backdrop-blur-sm transition-opacity dark:bg-neutral-900/50"><ul class="mx-auto flex w-full max-w-7xl list-none flex-col overflow-visible px-6 py-6 text-end sm:px-14 sm:py-10 sm:pt-10 md:px-24 lg:px-32"><li class=mb-1><span class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class="group mb-1"><a href=/posts/ title=Posts onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">blog</span></a></li><li class="group mb-1"><a href=/code/ title=Code onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">code</span></a></li><li class="group mb-1"><a href=/talks/ title=slides onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">slides</span></a></li><li class="group mb-1"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></div></label><ul class="hidden list-none flex-row text-end sm:flex"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/posts/ title=Posts><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">blog</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/code/ title=Code><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">code</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/talks/ title=slides><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">slides</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><button id=search-button-2 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="hidden inline"><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=https://masterskepticista.github.io/></a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=https://masterskepticista.github.io/posts/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class="hidden inline"><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=https://masterskepticista.github.io/posts/sgemm/>Optimizing a GEMM from first principles</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Optimizing a GEMM from first principles</h1><div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2025-07-14 00:00:00 +0000 UTC">14 July 2025</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">15 mins</span>
<span class=ps-2><span class=flex><span class="ms-1 rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Draft</span></span></span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 lg:sticky lg:top-10 print:hidden"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#roofline-analysis>Roofline Analysis</a></li><li><a href=#memory-layout>Memory Layout</a></li><li><a href=#kernel-1-loop-reorder>Kernel 1: Loop-reorder</a><ul><li><a href=#implicit-vectorization>Implicit Vectorization</a></li></ul></li><li><a href=#kernel-2-cache-blocking>Kernel 2: Cache blocking</a><ul><li><a href=#performance-ceiling>Performance ceiling</a></li><li><a href=#forcing-512-bit-vector-widths>Forcing 512-bit vector widths</a></li></ul></li><li><a href=#kernel-3-outer-product>Kernel 3: Outer Product</a><ul><li><a href=#matrix-multiply-as-an-outer-product>Matrix-multiply as an outer product</a></li><li><a href=#outer-product-using-registers>Outer Product using Registers</a></li><li><a href=#optimal-tile-size>Optimal Tile Size</a></li></ul></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><div class="flex rounded-md bg-primary-100 px-4 py-3 dark:bg-primary-900"><span class="pe-3 text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 0C114.6.0.0 114.6.0 256s114.6 256 256 256 256-114.6 256-256S397.4.0 256 0zm0 128c17.67.0 32 14.33 32 32 0 17.67-14.33 32-32 32s-32-14.3-32-32 14.3-32 32-32zm40 256h-80c-13.2.0-24-10.7-24-24s10.75-24 24-24h16v-64h-8c-13.25.0-24-10.75-24-24s10.8-24 24-24h32c13.25.0 24 10.75 24 24v88h16c13.25.0 24 10.75 24 24s-10.7 24-24 24z"/></svg>
</span></span><span class=dark:text-neutral-300>Work-in-progress. Code available <a href=https://github.com/masterskepticista/sgemm.c target=_blank rel=noreferrer>here</a>.</span></div><p>This is a worklog on optimizing a single-precision generalized matrix-multiply (GEMM) kernel in C to land close to Intel MKL performance. In the process of learning this for myself, I found the following sources really helpful. Building on the following, this article aims to approach the design decisions of GEMM at the level of a chip ISA.</p><ul><li><a href=https://en.algorithmica.org/hpc/algorithms/matmul/ target=_blank rel=noreferrer>Algorithmica: Matrix Multiplication</a></li><li><a href=https://salykova.github.io/gemm-cpu target=_blank rel=noreferrer>Advanced Matrix Multiplication on Multi-Core Processors</a></li><li><a href="https://youtu.be/VgSQ1GOC86s?si=G7VmTNu3uL5b0_8u" target=_blank rel=noreferrer>George Hotz | Programming | can you multiply a matrix? (noob lesson)</a></li></ul><h2 id=introduction class="relative group">Introduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>Let us start by describing the pointwise operation:</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/pointwise.png alt="Pointwise GEMM Operation" class="mx-auto my-0 rounded-md"></figure><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>void</span> <span class=nf>gemm_naive</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>C</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span><span class=n>A</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>M</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>K</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=nf>memset</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>M</span> <span class=o>*</span> <span class=n>N</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>));</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>M</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>i</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>It takes ~1.2 seconds for this kernel to multiply two 1000-size square matrices. This is absurdly slow for a CPU of this day and age. The Intel-MKL library, in comparison, finishes this operation in 13ms, about 100x faster. Before we start optimizing this naive kernel, lets take stock of how to reason about the performance of a kernel.</p><h2 id=roofline-analysis class="relative group">Roofline Analysis <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#roofline-analysis aria-label=Anchor>#</a></span></h2><p>System specs:</p><ul><li>Intel Xeon [Sapphire Rapids] 8488C @ 2.5GHz, 2 vCPUs<ul><li>Cache L1d: 48 KB/core | L2: 2 MB/core | L3: 105 MB/shared</li><li>ISA support: AVX-2 | AVX-512</li><li>Microarchitecture: Golden Cove<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></li></ul></li><li>4GB Memory, 10GB/s STREAM bandwidth (measured using <code>mbw</code>)</li><li>Ubuntu 24.04 LTS</li></ul><p>We will measure the performance of Generalized Matrix Multiply (GEMM) kernels in GFLOP/s (giga floating point operations per second). How many operations? GEMM involves <code>K</code> dot products across each row and column of <code>A</code> and <code>B</code> respectively to furnish each element of result matrix <code>C</code>.</p><p>$$
A^{M \times K} \times B^{K \times N} = 2 \cdot M \cdot N \cdot (K - 1) \approx 2 \cdot MNK
$$</p><p>We apply a scaling factor of 2 because we count multiply and adds as two separate ops. For equal matrix dimensions, this is roughly \(2N^3\) FLOPs. Assuming single precision floats, each input matrix <code>A</code>, <code>B</code> of size \(4 \cdot N^2\) bytes will be read once. Output matrix of size \(4 \cdot N^2\) bytes will be read and written back to memory. Therefore any kernel will read/write at least \(4 \cdot (4 \cdot N^2) \) bytes.</p><p>It is clear that total math ops grow faster than total memory read/writes. This ratio of floating point ops per byte of data moved is called the <strong>arithmetic intensity</strong> of an operation. For GEMM, arithmetic intensity \(\alpha\) is:</p><p>$$
\alpha = \frac{\text{ \char"0023 operations }}{\text{ \char"0023 bytes transferred }} = \frac{2N^3}{16N^2} = \frac{N}{8} \text{ FLOPs/byte }
$$</p><p>This means that as matrix sizes grow, GEMM operation becomes compute-bound. In fact, if we know the compute and memory bandwidth of a machine, we can find the machine&rsquo;s &lsquo;ridge point&rsquo;. Any kernel that uses less FLOPs/byte from the ridge point is said to be <em>memory-bound</em>, and vice versa.</p><p>Intel Golden Cove core has two FMA (fused multiply-add) units that can operate on 256-bit width vectors simultaneously. In single precision, this means 8 floats per vector. Each core also has 16 registers that are accessible in a single clock cycle. Registers sit on top of the memory hierarchy. FMA units have a latency of 4 clock cycles, and a throughput of 2 IPC<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. GEMM being an arithmetic demanding operation, we care only about FMA streaming throughput. The first-dispatch latency therefore is not relevant. This gives us enough information to calculate the compute bandwidth:</p><p>$$
2 \text{ ops } \times 2 \text{ IPC } \times 8 \text{ floats/cycle } \times 2.5 \text{ GHz } = 80 \text{ GFLOP/s }
$$</p><p>The DRAM bandwidth on our setup is 10GB/s per thread from a simple <code>mbw</code> benchmark. Therefore, the ridge point \(\gamma\) of this CPU across DRAM is:</p><p>$$
\gamma = \frac{\text{compute BW}}{\text{memory BW}} = 8 \text{ FLOPs/byte }
$$</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/hierarchy_roofline.png alt="Memory Hierarchy Roofline Model" class="mx-auto my-0 rounded-md"></figure><p>In practice, the ridge point depends on cache reuse, branching, and other instruction overheads. For instance, if we manage to keep the entire working set of a GEMM operation within cache boundary (which we will see soon with a cache-blocked GEMM kernel), the arithmetic intensity necessary to saturate compute units is quite lower. Here is a roofline model of the entire memory hierarchy and the corresponding ridge points. The focus of this worklog is on single-threaded operations only.</p><h2 id=memory-layout class="relative group">Memory Layout <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#memory-layout aria-label=Anchor>#</a></span></h2><p>As mentioned earlier, our arrays store floats in a row-major order, i.e., elements of a row are laid out consecutively. CPUs fetch contiguous blocks of memory (called a cache line) in the hope that consecutive memory elements will be needed for further processing. If a computation does not utilize all items in a cache line optimally, CPU cycles are wasted.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/row_major.png alt="Row-Major Memory Layout" class="mx-auto my-0 rounded-md"></figure><p>This gives us a couple of observations:</p><ul><li>Innermost loop iterates the fastest, over dimension <code>K</code>.</li><li>Array <code>A[M * K]</code> has <code>K</code> columns, with each element <code>A[i][k]</code> consecutively laid out in memory. Therefore, iteration over <code>K</code> is cache-friendly.</li><li>Array <code>B[K * N]</code> has <code>K</code> rows, each element <code>B[k][j]</code> requires jumping an entire row of <code>N</code> elements in memory. This results in a poor cache utilization.</li><li>Array <code>C[M * N]</code> has <code>j</code> as the fastest moving dimension, i.e., the second loop. It is consecutively laid out in memory, and is cache-friendly.</li></ul><h2 id=kernel-1-loop-reorder class="relative group">Kernel 1: Loop-reorder <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-1-loop-reorder aria-label=Anchor>#</a></span></h2><p>Iterating over rows of <code>B</code> is the problem. Notice that the nested for-loops are order-independent, and array <code>C</code> does not depend on dimension <code>K</code>. Therefore, we can reorder the loops such that iterating over <code>K</code> dimension is slower, and hence less costly for <code>B</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=ln> 1</span><span class=cl><span class=cm>/** Basic loop-reordered, pointwise GEMM kernel. */</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=kt>void</span> <span class=nf>gemm_loop_reorder</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>__restrict</span> <span class=n>C</span><span class=p>,</span> 
</span></span><span class=line><span class=ln> 3</span><span class=cl>                        <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>__restrict</span> <span class=n>A</span><span class=p>,</span> 
</span></span><span class=line><span class=ln> 4</span><span class=cl>                        <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>__restrict</span> <span class=n>B</span><span class=p>,</span> 
</span></span><span class=line><span class=ln> 5</span><span class=cl>                        <span class=kt>int</span> <span class=n>M</span><span class=p>,</span> 
</span></span><span class=line><span class=ln> 6</span><span class=cl>                        <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> 
</span></span><span class=line><span class=ln> 7</span><span class=cl>                        <span class=kt>int</span> <span class=n>K</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln> 8</span><span class=cl>  <span class=nf>memset</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>M</span> <span class=o>*</span> <span class=n>N</span> <span class=o>*</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>));</span>
</span></span><span class=line><span class=ln> 9</span><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>M</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class="line hl"><span class=ln>10</span><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class="line hl"><span class=ln>11</span><span class=cl>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>i</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=ln>14</span><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=ln>15</span><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=ln>16</span><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>By swapping <code>j &lt;-> k</code>, we retain cache-friendliness of <code>A</code> and <code>C</code>, while reusing the element <code>B[k][j]</code> for <code>N</code> iterations before incurring a cache miss. We still incur the same number of misses. We are simply amortizing the cost of each cache-miss by reusing the fetched element as long as possible.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/loop_reorder.png alt="SGEMM Benchmark" class="mx-auto my-0 rounded-md"><figcaption class=text-center>SGEMM Benchmark</figcaption></figure><p>On small matrices, this simple tweak boosts our GFLOP/s by 10-25x, saturating on the lower end as matrices grow large. What explains this jump? Can performance be sustained over large matrices?</p><h3 id=implicit-vectorization class="relative group">Implicit Vectorization <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implicit-vectorization aria-label=Anchor>#</a></span></h3><p>Even though our loop-reordered kernel defines scalar operations, the order of loop enables the compiler (with <code>-O2</code> flag) to fuse scalar operations into vector FMA (fused-multiply-add) instructions. We can see this in the <a href=https://godbolt.org/z/5aaeYMh67 target=_blank rel=noreferrer>disassembly</a> of our kernel.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-asm data-lang=asm><span class=line><span class=cl><span class=nl>.LBB0_19:</span>
</span></span><span class=line><span class=cl>    <span class=nf>vmovups</span> <span class=no>ymm2</span><span class=p>,</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>rsi</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nf>vmovups</span> <span class=no>ymm3</span><span class=p>,</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>rsi</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=err>+</span> <span class=mi>32</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nf>vmovups</span> <span class=no>ymm4</span><span class=p>,</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>rsi</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=err>+</span> <span class=mi>64</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nf>vmovups</span> <span class=no>ymm5</span><span class=p>,</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>rsi</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=err>+</span> <span class=mi>96</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nf>vfmadd213ps</span>     <span class=no>ymm2</span><span class=p>,</span> <span class=no>ymm1</span><span class=p>,</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>r14</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=p>-</span> <span class=mi>64</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nf>vfmadd213ps</span>     <span class=no>ymm3</span><span class=p>,</span> <span class=no>ymm1</span><span class=p>,</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>r14</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=p>-</span> <span class=mi>32</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nf>vfmadd213ps</span>     <span class=no>ymm4</span><span class=p>,</span> <span class=no>ymm1</span><span class=p>,</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>r14</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nf>vfmadd213ps</span>     <span class=no>ymm5</span><span class=p>,</span> <span class=no>ymm1</span><span class=p>,</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>r14</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=err>+</span> <span class=mi>32</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nf>vmovups</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>r14</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=p>-</span> <span class=mi>64</span><span class=p>],</span> <span class=no>ymm2</span>
</span></span><span class=line><span class=cl>    <span class=nf>vmovups</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>r14</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=p>-</span> <span class=mi>32</span><span class=p>],</span> <span class=no>ymm3</span>
</span></span><span class=line><span class=cl>    <span class=nf>vmovups</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>r14</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span><span class=p>],</span> <span class=no>ymm4</span>
</span></span><span class=line><span class=cl>    <span class=nf>vmovups</span> <span class=no>ymmword</span> <span class=no>ptr</span> <span class=p>[</span><span class=no>r14</span> <span class=err>+</span> <span class=mi>4</span><span class=p>*</span><span class=no>r10</span> <span class=err>+</span> <span class=mi>32</span><span class=p>],</span> <span class=no>ymm5</span>
</span></span></code></pre></div><p>This core has three kinds of vector registers: 128-wide <code>xmm</code> (first appeared as SSE<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>), 256-wide <code>ymm</code> (the classic AVX-2 intrinsics) and 512-wide <code>zmm</code> (new, AVX-512 intrinsics) registers. There are 16 <code>ymm</code> registers, each of which can hold 8 single-precision floats, and 32 <code>zmm</code> registers, each of which can hold 16 single-precision floats. Both <code>xmm</code> and <code>ymm</code> registers are a subset of the full 512-wide <code>zmm</code> registers to maintain backward compatibility.</p><p>When <code>ymm</code> registers are used, the performance ceiling halves to 80 GFLOP/s since half of the maximum possible vector width is wasted on every clock. On small matrices, this loop-reordered kernel is an order of magnitude faster because the active blocks fit within the L2 cache boundary. As the matrix size grows, performance plateaus until active blocks fit L3. For even larger matrices, the active blocks exceed cache boundary, and require multiple read/writes into the main memory.</p><h2 id=kernel-2-cache-blocking class="relative group">Kernel 2: Cache blocking <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-2-cache-blocking aria-label=Anchor>#</a></span></h2><p>Cache size is limited. As matrix dimensions grow, there is a possibility of older cache lines being &rsquo;evicted&rsquo; to fetch elements for the next iteration. This leads to wasteful load/stores and lower arithmetic intensity for large matrix sizes. We solve this by slicing each of the three dimensions into &rsquo;tiles&rsquo;, and executing smaller, cache-friendly matrix multiplies on those tiles. Tuned how?</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/tiling.png alt=Tiling class="mx-auto my-0 rounded-md"><figcaption class=text-center>Tiling</figcaption></figure><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=ln> 1</span><span class=cl><span class=cm>/** Cache-blocking across dimensions. */</span>
</span></span><span class=line><span class=ln> 2</span><span class=cl><span class=cp>#define TILE_K 128
</span></span></span><span class=line><span class=ln> 3</span><span class=cl><span class=cp>#define TILE_N 2048
</span></span></span><span class=line><span class=ln> 4</span><span class=cl><span class=cp>#define TILE_M 1024
</span></span></span><span class=line><span class=ln> 5</span><span class=cl>
</span></span><span class=line><span class=ln> 6</span><span class=cl><span class=kt>void</span> <span class=nf>gemm_cache_blocking</span><span class=p>(</span><span class=kt>float</span><span class=o>*</span> <span class=n>__restrict</span> <span class=n>C</span><span class=p>,</span> 
</span></span><span class=line><span class=ln> 7</span><span class=cl>                          <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>__restrict</span> <span class=n>A</span><span class=p>,</span> 
</span></span><span class=line><span class=ln> 8</span><span class=cl>                          <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>__restrict</span> <span class=n>B</span><span class=p>,</span> 
</span></span><span class=line><span class=ln> 9</span><span class=cl>                          <span class=kt>int</span> <span class=n>M</span><span class=p>,</span> 
</span></span><span class=line><span class=ln>10</span><span class=cl>                          <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> 
</span></span><span class=line><span class=ln>11</span><span class=cl>                          <span class=kt>int</span> <span class=n>K</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>12</span><span class=cl>  <span class=nf>memset</span><span class=p>(</span><span class=n>C</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>)</span> <span class=o>*</span> <span class=n>M</span> <span class=o>*</span> <span class=n>N</span><span class=p>);</span>
</span></span><span class=line><span class=ln>13</span><span class=cl>
</span></span><span class=line><span class=ln>14</span><span class=cl>  <span class=c1>// Tile across each dimension
</span></span></span><span class=line><span class=ln>15</span><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>M</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=n>TILE_M</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>16</span><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>mc</span> <span class=o>=</span> <span class=nf>min</span><span class=p>(</span><span class=n>TILE_M</span><span class=p>,</span> <span class=n>M</span> <span class=o>-</span> <span class=n>i</span><span class=p>);</span>
</span></span><span class=line><span class=ln>17</span><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>k</span> <span class=o>+=</span> <span class=n>TILE_K</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>18</span><span class=cl>      <span class=k>const</span> <span class=kt>int</span> <span class=n>kc</span> <span class=o>=</span> <span class=nf>min</span><span class=p>(</span><span class=n>TILE_K</span><span class=p>,</span> <span class=n>K</span> <span class=o>-</span> <span class=n>k</span><span class=p>);</span>
</span></span><span class=line><span class=ln>19</span><span class=cl>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span> <span class=o>+=</span> <span class=n>TILE_N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>20</span><span class=cl>        <span class=k>const</span> <span class=kt>int</span> <span class=n>nc</span> <span class=o>=</span> <span class=nf>min</span><span class=p>(</span><span class=n>TILE_N</span><span class=p>,</span> <span class=n>N</span> <span class=o>-</span> <span class=n>j</span><span class=p>);</span>
</span></span><span class=line><span class=ln>21</span><span class=cl>
</span></span><span class=line><span class=ln>22</span><span class=cl>        <span class=c1>// Update partials on each tile
</span></span></span><span class=line><span class=ln>23</span><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>ir</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>ir</span> <span class=o>&lt;</span> <span class=n>mc</span><span class=p>;</span> <span class=n>ir</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>24</span><span class=cl>          <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>p</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>p</span> <span class=o>&lt;</span> <span class=n>kc</span><span class=p>;</span> <span class=n>p</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>25</span><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>jc</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>jc</span> <span class=o>&lt;</span> <span class=n>nc</span><span class=p>;</span> <span class=n>jc</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=ln>26</span><span class=cl>              <span class=n>C</span><span class=p>[(</span><span class=n>i</span> <span class=o>+</span> <span class=n>ir</span><span class=p>)</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=p>(</span><span class=n>j</span> <span class=o>+</span> <span class=n>jc</span><span class=p>)]</span> <span class=o>+=</span> 
</span></span><span class=line><span class=ln>27</span><span class=cl>              <span class=n>A</span><span class=p>[(</span><span class=n>i</span> <span class=o>+</span> <span class=n>ir</span><span class=p>)</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=p>(</span><span class=n>k</span> <span class=o>+</span> <span class=n>p</span><span class=p>)]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[(</span><span class=n>k</span> <span class=o>+</span> <span class=n>p</span><span class=p>)</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=p>(</span><span class=n>j</span> <span class=o>+</span> <span class=n>jc</span><span class=p>)];</span>
</span></span><span class=line><span class=ln>28</span><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=ln>29</span><span class=cl>          <span class=p>}</span>
</span></span><span class=line><span class=ln>30</span><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=ln>31</span><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=ln>32</span><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=ln>33</span><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=ln>34</span><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>With cache-blocking, performance is consistent across all matrix sizes. The disassembly of this kernel is same as before. This is expected because the same instructions now run on &rsquo;tiles&rsquo; of matrices.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/cache_blocking.png alt="SGEMM Benchmark" class="mx-auto my-0 rounded-md"><figcaption class=text-center>SGEMM Benchmark</figcaption></figure><h3 id=performance-ceiling class="relative group">Performance ceiling <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#performance-ceiling aria-label=Anchor>#</a></span></h3><p>So our kernel uses 256-bit FMAs, and cache-blocking to sustain GFLOP/s. Recall from our roofline analysis, the performance ceiling is 80 GFLOP/s. To understand the reason behind saturation at 40 GFLOP/s, review the disassembly:</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/fma_load_stores.png alt="U-op count for the GEMM hot loop." class="mx-auto my-0 rounded-md"></figure><p>From the Golden Cove <a href=https://cdrdv2-public.intel.com/821613/355308-Optimization-Reference-Manual-050-Changes-Doc.pdf target=_blank rel=noreferrer>microarchitecture</a>, we find the following uOp capacities:</p><table><thead><tr><th>Op</th><th>Capacity (per cycle)</th><th>Requirement</th><th>Cycles</th></tr></thead><tbody><tr><td>Loads</td><td>\(3 \times 256\)</td><td>\(8 \times 256\)</td><td>\(2.67\)</td></tr><tr><td>Stores</td><td>\(2 \times 256\)</td><td>\(4 \times 256\)</td><td>\(2\)</td></tr><tr><td>FMAs</td><td>\(2\)</td><td>\(4\)</td><td>\(2\)</td></tr></tbody></table><div class="flex rounded-md bg-primary-100 px-4 py-3 dark:bg-primary-900"><span class="pe-3 text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 0C114.6.0.0 114.6.0 256s114.6 256 256 256 256-114.6 256-256S397.4.0 256 0zm0 128c17.67.0 32 14.33 32 32 0 17.67-14.33 32-32 32s-32-14.3-32-32 14.3-32 32-32zm40 256h-80c-13.2.0-24-10.7-24-24s10.75-24 24-24h16v-64h-8c-13.25.0-24-10.75-24-24s10.8-24 24-24h32c13.25.0 24 10.75 24 24v88h16c13.25.0 24 10.75 24 24s-10.7 24-24 24z"/></svg>
</span></span><span class=dark:text-neutral-300>A 32-bit scalar from <code>A</code> is broadcasted to <code>ymm1</code> and reused for the entire iteration. The load cost is negligible compared to the rest, hence ignored in calculations.</span></div><p>Loads take approximately 2.67 cycles. FMAs execute as soon as the operands are ready, and hence the load ops &lsquo;mask&rsquo; the 2 cycles consumed by FMAs. Stores take 2 cycles after FMAs retire. So the percentage of &lsquo;useful&rsquo; multiply-add work:</p><p>$$
\frac{2 \text{ FMA}}{2.67 \text{ loads } + 2 \text{ stores}} = \frac{2}{4.67} \approx 0.43
$$</p><p>If FMA widths are 256-bit as in the disassembly, our performance ceiling with this kernel is \(0.43 \times 80 = 34.4 \text{ GFLOP/s}\). This matches our expected GFLOP/s from the kernel.</p><h3 id=forcing-512-bit-vector-widths class="relative group">Forcing 512-bit vector widths <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#forcing-512-bit-vector-widths aria-label=Anchor>#</a></span></h3><p>We can supply a compiler flag <code>-mprefer-vector-width=512</code>. Our requirements now look as follows:</p><table><thead><tr><th>Op</th><th>Capacity (per cycle)</th><th>Requirement</th><th>Cycles</th></tr></thead><tbody><tr><td>Loads</td><td>\(2 \times 512\)</td><td>\(8 \times 512\)</td><td>\(4\)</td></tr><tr><td>Stores</td><td>\(1 \times 512\)</td><td>\(4 \times 512\)</td><td>\(4\)</td></tr><tr><td>FMAs</td><td>\(2\)</td><td>\(4\)</td><td>\(2\)</td></tr></tbody></table><p>The percentage of &lsquo;useful&rsquo; multiply-add work:</p><p>$$
\frac{2 \text{ FMA}}{4 \text{ loads } + 4 \text{ stores}} = \frac{2}{8} \approx 0.25
$$</p><p>Our performance ceiling with this flag is \(0.25 \times 160 = 40 \text{ GFLOP/s}\). It is a marginal improvement, because golden cove supports 2 loads/cycle when fetching 512-bit memory (compared to 3 loads/cycle for 256-bits). In practice, this flag gives us a very close GFLOP/s to what we predict. Neat!</p><h2 id=kernel-3-outer-product class="relative group">Kernel 3: Outer Product <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-3-outer-product aria-label=Anchor>#</a></span></h2><p>So far we have been looking at matrix multiplication as repeated dot products between <strong>rows</strong> of <code>A</code> and <strong>columns</strong> of <code>B</code>:
$$
C_{ij} = \sum_{k=1}^K A_{ik} \cdot B_{kj}
$$</p><p>Dot products are inefficient on hardware for the following reasons:</p><ul><li><strong>Frequent Load/Stores for <code>C</code></strong>: Tiles of <code>C</code> are read and written repeatedly. This is clear from our disassembly analysis. The useful FMA work is capped at 43%.</li><li><strong>Poor Register Utilization</strong>: Registers are the fastest to access in the memory hierarchy. Vector intrinsics on modern cores like Golden Cove have 16 vector registers (32 in AVX-512). The dot-product loop uses about 6-7 registers for temporary accumulations.</li><li><strong>Arithmetic Intensity</strong>: GEMM gets more compute intense with size. Our current implementation is load/store bound at large sizes. We need to amortize the cost of load/stores with more arithmetic work.</li></ul><h3 id=matrix-multiply-as-an-outer-product class="relative group">Matrix-multiply as an outer product <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#matrix-multiply-as-an-outer-product aria-label=Anchor>#</a></span></h3><p>Matrix multiply can be rewritten as a cumuluative outer-product between <strong>columns</strong> of <code>A</code> and <strong>rows</strong> of <code>B</code>:
$$
C = A \times B = \sum_{k=0}^{K-1} A_{:,k} \otimes B_{k,:}
$$</p><p>Here:</p><ul><li>\(A_{:,k}\) is the \(k\)-th column of <code>A</code> (an \(M \times 1\) vector).</li><li>\(B_{k,:}\) is the \(k\)-th row of <code>B</code> (a \(1 \times N\) vector).</li></ul><p>Their outer product (\(\otimes\)) produces an \(M \times N\) matrix where each element is \(A_{i,k} \cdot B_{k,j}\).
Summing these over all \(k\) gives the full \(C\).</p><p>This is algebraically identical to the dot-product view but shifts the focus: instead of accumulating inward along \(k\) for each fixed \((i,j)\), we are broadcasting outward from each \(k\), adding a full &ldquo;layer&rdquo; to \(C\) at a time.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/outer_product_view.png alt="Outer Product view of A, B, C." class="mx-auto my-0 rounded-md"></figure><p>What motivates this reformulation?</p><ul><li><strong>Register Reuse</strong>: In the outer-product view, we can load slices of \(A_{:,k}\) and \(B_{k,:}\) into registers, compute their outer product, and accumulate it directly into a register-resident tile of \(C\). Registers are plentiful (16 YMMs can hold 128 floats total), so we can &ldquo;block&rdquo; a small \(\text{MR} \times \text{NR}\) tile of \(C\) using multiple ZMMs.</li><li><strong>Load/Store Amortization</strong>: After several updates over \(k\), we store the \(\text{MR} \times \text{NR}\) tile of \(C\) back to memory. This amortizes load/store costs over more FMAs.</li><li><strong>Higher Arithmetic Intensity</strong>: By accumulating multiple outer products in registers, the ratio of computations to memory accesses increases.</li></ul><h3 id=outer-product-using-registers class="relative group">Outer Product using Registers <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#outer-product-using-registers aria-label=Anchor>#</a></span></h3><p>CPUs do not have an intrinsic for vector outer product, which means we need to compute one iteratively using vector FMAs.</p><p>Consider loading \(\text{MR}\) scalars from \(A\) across the column, and \(\text{NR}\) scalars from \(B\) across the row.</p><div class="flex rounded-md bg-primary-100 px-4 py-3 dark:bg-primary-900"><span class="pe-3 text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 0C114.6.0.0 114.6.0 256s114.6 256 256 256 256-114.6 256-256S397.4.0 256 0zm0 128c17.67.0 32 14.33 32 32 0 17.67-14.33 32-32 32s-32-14.3-32-32 14.3-32 32-32zm40 256h-80c-13.2.0-24-10.7-24-24s10.75-24 24-24h16v-64h-8c-13.25.0-24-10.75-24-24s10.8-24 24-24h32c13.25.0 24 10.75 24 24v88h16c13.25.0 24 10.75 24 24s-10.7 24-24 24z"/></svg>
</span></span><span class=dark:text-neutral-300>You may (rightly) wonder that accesses across \(A\) are not cache-friendly. In practice, we transpose a tile of <code>A</code> into a buffer, which gets passed into the outer-product microkernel. Transposed <code>A</code> is cache-friendly and reuses the same for <code>K</code> outer products. Check code for details.</span></div><p>We iteratively broadcast + FMA each of the scalars from \(A\) to vectors of \(B\), cumulatively storing the result in an \(\text{MR} \times \text{NR}\) register tile of \(C\).</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/sgemm/outer_product_loop.png alt="Outer Product view of A, B, C." class="mx-auto my-0 rounded-md"></figure><p>Here is a pseudocode of the inner loop:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mathematica data-lang=mathematica><span class=line><span class=cl><span class=o>&lt;!--</span><span class=w> </span><span class=n>m</span><span class=o>=</span><span class=n>MR</span><span class=w> </span><span class=n>scalars</span><span class=w> </span><span class=n>of</span><span class=w> </span><span class=n>A</span><span class=w> </span><span class=o>--&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=o>&lt;!--</span><span class=w> </span><span class=n>n</span><span class=o>=</span><span class=n>NR</span><span class=o>/</span><span class=mi>8</span><span class=w> </span><span class=n>vectors</span><span class=w> </span><span class=n>of</span><span class=w> </span><span class=n>B</span><span class=w> </span><span class=o>--&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=n>a</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=n>b</span><span class=p>[</span><span class=n>NR</span><span class=o>/</span><span class=mi>8</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=n>c</span><span class=p>[</span><span class=n>MR</span><span class=p>][</span><span class=n>NR</span><span class=o>/</span><span class=mi>8</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=p>{}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=o>&lt;!--</span><span class=w> </span><span class=n>Load</span><span class=w> </span><span class=n>tile</span><span class=w> </span><span class=n>from</span><span class=w> </span><span class=n>C</span><span class=w> </span><span class=o>--&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=n>for</span><span class=w> </span><span class=n>m</span><span class=w> </span><span class=n>in</span><span class=w> </span><span class=n>MR</span><span class=err>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>for</span><span class=w> </span><span class=n>n</span><span class=w> </span><span class=n>in</span><span class=w> </span><span class=n>NR</span><span class=o>/</span><span class=mi>8</span><span class=err>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>c</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=n>n</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>load</span><span class=p>(</span><span class=n>C</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=n>n</span><span class=p>])</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=o>&lt;!--</span><span class=w> </span><span class=n>Loop</span><span class=w> </span><span class=n>over</span><span class=w> </span><span class=n>inner</span><span class=w> </span><span class=n>dimension</span><span class=w> </span><span class=o>--&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=n>for</span><span class=w> </span><span class=n>p</span><span class=w> </span><span class=n>in</span><span class=w> </span><span class=n>K</span><span class=err>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>b</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span><span class=w> </span><span class=err>...</span><span class=p>,</span><span class=w> </span><span class=n>b</span><span class=p>[</span><span class=n>NR</span><span class=o>/</span><span class=mi>8</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>load</span><span class=p>(</span><span class=n>B</span><span class=p>[</span><span class=err>:</span><span class=n>NR</span><span class=p>])</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=o>&lt;!--</span><span class=w> </span><span class=n>One</span><span class=w> </span><span class=n>iteration</span><span class=w> </span><span class=p>(</span><span class=n>hot</span><span class=w> </span><span class=n>FMA</span><span class=w> </span><span class=n>loop</span><span class=p>)</span><span class=w> </span><span class=o>--&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>for</span><span class=w> </span><span class=n>m</span><span class=w> </span><span class=n>in</span><span class=w> </span><span class=n>MR</span><span class=err>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>a</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>broadcast</span><span class=p>(</span><span class=n>load</span><span class=p>(</span><span class=n>A</span><span class=p>[</span><span class=n>m</span><span class=p>]))</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=o>&lt;!--</span><span class=w> </span><span class=n>Outer</span><span class=w> </span><span class=n>product</span><span class=w> </span><span class=n>within</span><span class=w> </span><span class=n>registers</span><span class=w> </span><span class=o>--&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>for</span><span class=w> </span><span class=n>n</span><span class=w> </span><span class=n>in</span><span class=w> </span><span class=n>NR</span><span class=o>/</span><span class=mi>8</span><span class=err>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=n>c</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=n>n</span><span class=p>]</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>fma</span><span class=p>(</span><span class=n>a</span><span class=p>,</span><span class=w> </span><span class=n>b</span><span class=p>[</span><span class=n>n</span><span class=p>],</span><span class=w> </span><span class=n>c</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=n>n</span><span class=p>])</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>A</span><span class=w> </span><span class=o>+=</span><span class=w> </span><span class=n>MR</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>B</span><span class=w> </span><span class=o>+=</span><span class=w> </span><span class=n>NR</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=o>&lt;!--</span><span class=w> </span><span class=n>Store</span><span class=w> </span><span class=n>back</span><span class=w> </span><span class=n>to</span><span class=w> </span><span class=n>C</span><span class=w> </span><span class=o>--&gt;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=n>for</span><span class=w> </span><span class=n>m</span><span class=w> </span><span class=n>in</span><span class=w> </span><span class=n>MR</span><span class=err>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=n>for</span><span class=w> </span><span class=n>n</span><span class=w> </span><span class=n>in</span><span class=w> </span><span class=n>NR</span><span class=o>/</span><span class=mi>8</span><span class=err>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>store</span><span class=p>(</span><span class=n>c</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=n>n</span><span class=p>],</span><span class=w> </span><span class=n>C</span><span class=p>[</span><span class=n>m</span><span class=p>][</span><span class=n>n</span><span class=p>])</span><span class=w>
</span></span></span></code></pre></div><h3 id=optimal-tile-size class="relative group">Optimal Tile Size <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#optimal-tile-size aria-label=Anchor>#</a></span></h3><p>When using <code>YMM</code> vector registers, we have a limit of 16. The scalars we load from \(B\) of size \(\text{NR}\) must be a multiple of 8 to fit in one register. Hence \(B\) vector will use \(\text{NR}/8\) registers. Each scalar from \(A\) uses 1 register: the scalar is broadcasted to the entire vector. The \(C\) accumulator fully resides in registers, requiring \(\text{MR} \times \text{NR}/8\) registers. Therefore, we need to satisfy the inequality:</p><p>$$ \text{MR} \cdot \frac{\text{NR}}{8} + \frac{\text{NR}}{8} + 1 \leq 16 $$</p><p>Since \(\text{MR} \ge 1\) and \(\text{NR} \ge 8\) is necessary, we have the following acceptable combinations:</p><p>$$
\begin{array}{|c|c|c|c|c|c|}
\hline
\text{MR} & \text{NR} & \text{YMM register ct.} & \text{Loads/iter (bytes)} & \text{FLOPs/iter} & \text{FLOPs/byte} \\
\hline
1 & 56 & 15 & 228 & 112 & 0.491 \\
2 & 40 & 16 & 168 & 160 & 0.952 \\
4 & 24 & 16 & 112 & 192 & 1.714 \\
6 & 16 & 15 & 88 & 192 & 2.182 \\
14 & 8 & 16 & 88 & 224 & 2.545 \\
\hline
\end{array}
$$</p><p>Only the \(6 \times 16\) and \(14 \times 8\) size micro-kernels are capable of saturating the core within L3 boundary (recall from the roofline plot, \(2.16 \text{ FLOPs/byte}\)), so we can discard other candidates. Of the two that remain, \(14 \times 8\) tile actually ends up being load bound. The <a href=https://godbolt.org/z/YMoEExxv8 target=_blank rel=noreferrer>disassembly</a> shows a memory broadcast on every FMA; compilers tend to generate memory-source FMAs instead of separating the load and broadcast into registers. As a result, even though the total number of bytes accessed is similar, each scalar requires its own load op during the FMA. This leads to roughly 15 load instructions per iteration (14 scalar loads plus one 256-bit vector load).</p><p>By contrast, the \(6 \times 16\) micro-kernel performs six scalar loads and two 256-bit vector loads, for a total of eight loads. This produces a much better balance between load throughput and FMA issue rate, allowing the kernel to approach core saturation. This explains the popular choice of \(6 \times 16\) in various BLAS libraries using AVX intrinsics.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://chipsandcheese.com/p/popping-the-hood-on-golden-cove target=_blank rel=noreferrer>Popping the Hood on Golden Cove, by Chester Lam</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Instructions per clock. Here it means that both FMA units can dispatch in parallel (<a href=https://chipsandcheese.com/p/a-peek-at-sapphire-rapids target=_blank rel=noreferrer>ref</a>).&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Streaming SIMD Extensions debuted with Pentium-III.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=flex><picture class="!mb-0 !mt-0 me-4 w-24 h-auto rounded-full"><img width=560 height=560 class="!mb-0 !mt-0 me-4 w-24 h-auto rounded-full" alt="Karan Shah" loading=lazy decoding=async src=https://masterskepticista.github.io/img/profile.png></picture><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Karan Shah</div><div class="text-sm text-neutral-700 dark:text-neutral-400">Sampling meaning from a non-stationary prior.</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://github.com/MasterSkepticista target=_blank aria-label=Github rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://twitter.com/elevated_quark target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href="https://scholar.google.com/citations?user=vEgcRTkAAAAJ&amp;hl=en" target=_blank aria-label=Google-Scholar rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg height="16" width="16" viewBox="0 0 512 512"><path fill="currentColor" d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64h0c1.7-3.6 3.6-7.2 5.6-10.7 4.4-7.6 9.4-14.7 15-21.3 27.4-32.6 68.5-53.3 114.4-53.3 33.6.0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0 512 202.7l-94.7 77.1z"/></svg></span></a>
<a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://stackoverflow.com/users/9230398/karan-shah target=_blank aria-label=Stack-Overflow rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 384 512"><path fill="currentColor" d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 3e2zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-2e2v39.7h2e2zm39.7 80H42.7V320h-40v160h359.5V320h-40z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://linkedin.com/in/karan-bhavesh-shah target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></a></div></div></div></div><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=https://masterskepticista.github.io/posts/early-opinions/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">An Opinion on opinions</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2026-01-07 00:00:00 +0000 UTC">7 January 2026</time>
</span></span></a></span><span><a class="group flex text-right" href=https://masterskepticista.github.io/posts/reduce-sum/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Embarrassingly Parallel Reduction in CUDA</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2025-02-19 00:00:00 +0000 UTC">19 February 2025</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article></main><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12" id=to-top hidden=true><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div><footer class="py-10 print:hidden"><nav class="pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex list-none flex-col sm:flex-row"><li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a href=/tags/ title=Tags><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">tags</span></a></li><li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a href=https://drive.google.com/file/d/1blFQIqhagkHfd3sqsjawCyPCvl0Evrtw/view title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">cv</span></a></li></ul></nav><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">If you copy this, may your WiFi become unstable.</p></div><div class="flex flex-row items-center"><div class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex h-12 w-12 items-center justify-center dark:hidden" title="Switch to dark appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="hidden h-12 w-12 items-center justify-center dark:flex" title="Switch to light appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://masterskepticista.github.io/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>