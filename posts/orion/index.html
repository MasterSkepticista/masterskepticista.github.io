<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#FFFFFF"><title>Data Parallelism using standard Ethernet &#183; MasterSkepticista</title>
<meta name=title content="Data Parallelism using standard Ethernet &#183; MasterSkepticista"><script type=text/javascript src=/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.e0c75b2446b3f8545af40deeeebc2b7899e8a610754fea305617e08710cee268.css integrity="sha256-4MdbJEaz+FRa9A3u7rwreJnophB1T+owVhfghxDO4mg="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.af5d9722112bedac95702865c340bcd6286c4e9b2c15ce26b531ea1fba974cb8.js integrity="sha256-r12XIhEr7ayVcChlw0C81ihsTpssFc4mtTHqH7qXTLg=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        Bag-of-tricks for multi-node training for the GPU Poor.
      
    "><link rel=canonical href=https://masterskepticista.github.io/posts/orion/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://masterskepticista.github.io/posts/orion/"><meta property="og:site_name" content="MasterSkepticista"><meta property="og:title" content="Data Parallelism using standard Ethernet"><meta property="og:description" content="Bag-of-tricks for multi-node training for the GPU Poor."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-05T00:00:00+00:00"><meta property="article:tag" content="Data Parallelism"><meta property="article:tag" content="Multi-Node"><meta property="article:tag" content="Jax"><meta property="article:tag" content="Tensorflow"><meta property="article:tag" content="Pytorch"><meta name=twitter:card content="summary"><meta name=twitter:title content="Data Parallelism using standard Ethernet"><meta name=twitter:description content="Bag-of-tricks for multi-node training for the GPU Poor."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Data Parallelism using standard Ethernet","headline":"Data Parallelism using standard Ethernet","abstract":"Bag-of-tricks for multi-node training for the GPU Poor.","inLanguage":"en","url":"https:\/\/masterskepticista.github.io\/posts\/orion\/","author":{"@type":"Person","name":"Karan Shah"},"copyrightYear":"2024","dateCreated":"2024-08-05T00:00:00\u002b00:00","datePublished":"2024-08-05T00:00:00\u002b00:00","dateModified":"2024-08-05T00:00:00\u002b00:00","keywords":["data parallelism","multi-node","jax","tensorflow","pytorch"],"mainEntityOfPage":"true","wordCount":"1632"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://masterskepticista.github.io/","name":"","position":1},{"@type":"ListItem","item":"https://masterskepticista.github.io/posts/","name":"Posts","position":2},{"@type":"ListItem","name":"Data Parallelism Using Standard Ethernet","position":3}]}</script><meta name=author content="Karan Shah"><link href=https://masterskepticista.github.io/ rel=me><link href=https://github.com/MasterSkepticista rel=me><link href="https://scholar.google.com/citations?user=vEgcRTkAAAAJ&amp;hl=en" rel=me><link href=https://linkedin.com/in/karan-bhavesh-shah rel=me><link href=https://stackoverflow.com/users/9230398/karan-shah rel=me><link href=https://twitter.com/elevated_quark rel=me><link type=text/css rel=stylesheet href=/lib/katex/katex.min.68e17230ccd917b97b7a2def38a8108918599d8aa4f580bfb8cce5e13d23e4de43dcaba5f9000553cb2c10d0d1300aabfe5c433a3305ebd752609f0762a63e59.css integrity="sha512-aOFyMMzZF7l7ei3vOKgQiRhZnYqk9YC/uMzl4T0j5N5D3Kul+QAFU8ssENDRMAqr/lxDOjMF69dSYJ8HYqY+WQ=="><script defer src=/lib/katex/katex.min.50f14e69d6a8da7128ae3b63974c544ed377c36d096b5e3750f114e84c89d668b9301d9b0ed3248969aa183aa2e3bc4d2c1e73d5dcb7d462890c45a18d424589.js integrity="sha512-UPFOadao2nEorjtjl0xUTtN3w20Ja143UPEU6EyJ1mi5MB2bDtMkiWmqGDqi47xNLB5z1dy31GKJDEWhjUJFiQ=="></script><script defer src=/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" onload=renderMathInElement(document.body)></script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>MasterSkepticista</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blog</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/code/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Code</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/talks/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Talks</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Data Parallelism using standard Ethernet</h1><div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2024-08-05 00:00:00 +0000 UTC">5 August 2024</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">8 mins</span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 lg:sticky lg:top-10 print:hidden"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#infrastructure>Infrastructure</a></li><li><a href=#goals>Goals</a></li><li><a href=#single-gpu-training-baseline>Single GPU Training: Baseline</a><ul><li><a href=#bandwidth-calculation>Bandwidth Calculation</a></li></ul></li><li><a href=#multi-gpu-training-baseline>Multi GPU Training: Baseline</a></li><li><a href=#1-reducing-communication>1. Reducing Communication</a></li><li><a href=#2-deferring-communication>2. Deferring Communication</a></li><li><a href=#3-faster-communication>3. Faster Communication</a></li><li><a href=#result>Result</a></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><div class="lead !mb-9 text-xl">Big model ask big money.</div><p>The limiting factor on the size of models that can be trained, can be summarized to data movement speeds - either within the GPU (HBM bandwidth) or across GPUs (collective ops bandwidth). A big part of model scaling with the number of accelerators is achieved by reducing communication overhead across them. This is why protocols like Infiniband/NVLink exist.</p><p>But can we get away without spending a fortune on 100G/400G NICs for training models across nodes? Turns out, under the right assumptions, we can.</p><h2 id=infrastructure class="relative group">Infrastructure <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#infrastructure aria-label=Anchor>#</a></span></h2><p>We had four<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> server blades each with the following spec:</p><ul><li>Dual socket Xeon 6258R (28C/56T per socket)</li><li>512GB DDR4 Memory</li><li>One RTX-3090 GPU</li><li>Intel X540 Dual-port 10GbE, one of the ports was utilized for Internet via a 1GbE link.</li></ul><h2 id=goals class="relative group">Goals <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#goals aria-label=Anchor>#</a></span></h2><p>We had to consolidate these servers into a multi-node training cluster:</p><ul><li>With above <strong>90%</strong> scaling factor.</li><li>With support for medium sized models (think ResNet-50/101 or ViT-S/B) up to 100M params.</li><li>Using 10G Ethernet only. Any other NIC would require a new switch and a new card.</li></ul><p>The constraint: spend $0.</p><h2 id=single-gpu-training-baseline class="relative group">Single GPU Training: Baseline <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#single-gpu-training-baseline aria-label=Anchor>#</a></span></h2><p>Our focus was on data-parallel training since we could fit all our models on a single GPU (modulo tuning the batch size).</p><p>Take ResNet50 for example. Training a ResNet50 on 90 epochs of ImageNet-1k on a single blade takes 48h in the default <code>TensorFloat32</code> precision. Since Ampere+ architectures support <code>bfloat16</code>, we could reduce data movement within the GPU, and saturate the Tensor Cores (Tesla architecture supports <code>float16</code>, but that requires gradient scaling in the training loop to avoid overflows. I won&rsquo;t cover that here given there exist plenty of guides online on how to use <code>float16</code>).</p><p>Here is a summary of changes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># JAX</span>
</span></span><span class=line><span class=cl><span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>jnp</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dense</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>jnp</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># TensorFlow (keras)</span>
</span></span><span class=line><span class=cl><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>mixed_precision</span><span class=o>.</span><span class=n>set_global_policy</span><span class=p>(</span><span class=s1>&#39;mixed_bfloat16&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Torch</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>autocast</span><span class=p>(</span><span class=n>device_type</span><span class=o>=</span><span class=n>device_type</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=o>...</span>
</span></span></code></pre></div><p>We wrote our training pipelines in TensorFlow + JAX for two main reasons:</p><ul><li>Fine-grained data ETL tuning with <code>tf.data</code>, and</li><li>Higher GPU throughput via XLA. This frees us from low-level compiler optimizations. (If you are a PyTorch user, <code>torch.compile</code> will do this for you)</li></ul><p>Half-precision matmuls got us down to 25h while hitting the same Top-1 score of 76.3%. We clocked a per-step time \(T_s=230\text{ms}\) (i.e. time taken per forward/backward pass of a batch). This was a reasonable single GPU baseline.</p><p>At this point we can extrapolate to our &ldquo;ideal&rdquo; cluster: it would train a ResNet50 just <strong>below 6.3h with 100% scaling</strong> across four nodes.</p><h3 id=bandwidth-calculation class="relative group">Bandwidth Calculation <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#bandwidth-calculation aria-label=Anchor>#</a></span></h3><p>Training across nodes requires taking a global average of gradients across all GPUs on each backward pass. So each GPU would have to send and receive a full set of <code>gradient_size</code> data at each step.</p><p>$$
\text{gradient\_size} = \frac{\text{params\_size}}{1e^6} \times \text{bytes\_per\_param}
$$</p><p>For a ResNet50 with 25M parameters, <code>gradient_size</code> is roughly 100MB per step, per GPU. Since each GPU needs a full copy of globally averaged gradients - a naive algorithm would require the lead host to <code>fetch</code> and <code>broadcast</code> 100MB data to/from each GPU. This would create a massive bottleneck on the main host, since the communication time would grow linearly on the number of GPUs.</p><p>Lucky for us, most implementations<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> of collectives today use the <code>RingAllReduce</code> algorithm, which amortizes the amount of transfers as number of GPUs increase, by communicating &lsquo;chunked&rsquo; gradients. In other words: data communicated per GPU reaches an asymptotic limit, independent of the number of GPUs in the cluster.</p><p>$$
\text{data\_per\_gpu} = 2 (N - 1) \frac{\text{gradient\_size}}{N} = \frac{3}{2} \times 100 \text{ MB}
$$</p><p>If you are interested in the proof, Gibiansky has a great article explaining the <a href=https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/ target=_blank rel=noreferrer><code>RingAllReduce</code></a> algorithm.</p><div class="flex rounded-md bg-primary-100 px-4 py-3 dark:bg-primary-900"><span class="pe-3 text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 0C114.6.0.0 114.6.0 256s114.6 256 256 256 256-114.6 256-256S397.4.0 256 0zm0 128c17.67.0 32 14.33 32 32 0 17.67-14.33 32-32 32s-32-14.3-32-32 14.3-32 32-32zm40 256h-80c-13.2.0-24-10.7-24-24s10.75-24 24-24h16v-64h-8c-13.25.0-24-10.75-24-24s10.8-24 24-24h32c13.25.0 24 10.75 24 24v88h16c13.25.0 24 10.75 24 24s-10.7 24-24 24z"/></svg>
</span></span><span class=dark:text-neutral-300>In complex topologies spanning thousands of GPUs, a <a href=https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/ target=_blank rel=noreferrer><code>HierarchicalAllReduce</code></a> algorithm scales better.</span></div><p>On our 4-node cluster with 10GbE bi-directional links, time spent in communication would be</p><p>$$
T_c = \frac{\text{data\_per\_gpu}}{\text{bandwidth}} = \frac{150}{1.25 \times 1024} = 0.11 \text{s.}
$$</p><p>So we would pay a fixed cost of 110ms each time, to synchronize gradients.</p><h2 id=multi-gpu-training-baseline class="relative group">Multi GPU Training: Baseline <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#multi-gpu-training-baseline aria-label=Anchor>#</a></span></h2><p>Lets start with a simple baseline that connects all 4 blades through a 10G switch. We can measure the time spent in computation and communication using Tensorboard profiler.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/orion/tb_prof_overview.png alt="Step profile of ResNet50" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Step profile of ResNet50</figcaption></figure><p>With a total forward + backward pass time \(T_s = 234\text{ms}\), we spend an additional \(T_c = 106\text{ms}\) in communication (&ldquo;NCCL&rdquo;), in line with our estimate above. Note that we already save time prefetching batches to the GPU by overlapping it with computation (&ldquo;Copy&rdquo; step).
With this information, we can calculate the scaling efficiency \(\eta\) of our cluster.</p><p>$$
\eta = \frac{T_s}{T_s + T_c} \approx 68.8%
$$</p><p>Now, our ResNet50 takes 9.1h to train (i.e., a \(2.74\times\) speedup over single GPU baseline). It is a sizeable jump, but notice that more than 1 GPU worth of our compute is spent idling.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/orion/baseline.png alt="Scaling efficiency baseline" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Scaling efficiency baseline</figcaption></figure><p>If we take a close look at the formulation of \(\eta\), we only have two ways to increase scaling efficiency from here:</p><ol><li>Reduce communication (\(T_c\)), and/or</li><li>Defer communication (by increasing \(T_s\)).</li></ol><p>We will now explore each optimization in detail.</p><h2 id=1-reducing-communication class="relative group">1. Reducing Communication <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#1-reducing-communication aria-label=Anchor>#</a></span></h2><p>Upto this point we communicate 25M <code>float32</code> values at the end of each step. One way to reduce communication could be by compressing gradients (lossy or otherwise). Here are our options:</p><ol><li>Cast gradients to <code>bfloat16</code>: No risk of overflow, but lossy due to high machine \(\epsilon\)<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</li><li>Cast gradients to <code>float16</code>: Risk of overflow, but lossless if renormalized.</li><li>Use a more intelligent gradient compression schema (like sparsity?).</li></ol><p>We&rsquo;ll stick with a simple technique that worked for us. We cast gradients to <code>bfloat16</code> during communication. We did not observe any loss in accuracy.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># JAX</span>
</span></span><span class=line><span class=cl><span class=n>grads</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>tree</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>g</span><span class=p>:</span> <span class=n>g</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>),</span> <span class=n>grads</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>grads</span> <span class=o>=</span> <span class=n>lax</span><span class=o>.</span><span class=n>pmean</span><span class=p>(</span><span class=n>grads</span><span class=p>,</span> <span class=n>axis_name</span><span class=o>=</span><span class=s2>&#34;batch&#34;</span><span class=p>)</span>  <span class=c1># AllReduce</span>
</span></span><span class=line><span class=cl><span class=n>grads</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>tree</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>g</span><span class=p>:</span> <span class=n>g</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>float32</span><span class=p>),</span> <span class=n>grads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Tensorflow (Keras)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compressed_aggregate_gradients</span><span class=p>(</span><span class=n>grads_and_vars</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;An override for `tf.optimizers.Optimizer.aggregate_gradients` method to 
</span></span></span><span class=line><span class=cl><span class=s2>  compress gradients before allreduce.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=n>grads</span><span class=p>,</span> <span class=nb>vars</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>grads_and_vars</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>grads</span> <span class=o>=</span> <span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>g</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float16</span><span class=p>)</span> <span class=k>for</span> <span class=n>g</span> <span class=ow>in</span> <span class=n>grads</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>grads</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>distribute</span><span class=o>.</span><span class=n>get_replica_context</span><span class=p>()</span><span class=o>.</span><span class=n>all_reduce</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=n>tf</span><span class=o>.</span><span class=n>distribute</span><span class=o>.</span><span class=n>ReduceOp</span><span class=o>.</span><span class=n>SUM</span><span class=p>,</span> <span class=n>grads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>grads</span> <span class=o>=</span> <span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>g</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span> <span class=k>for</span> <span class=n>g</span> <span class=ow>in</span> <span class=n>grads</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>grads_and_vars</span> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><span class=n>grads</span><span class=p>,</span> <span class=nb>vars</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>grads_and_vars</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>aggregate_gradients</span> <span class=o>=</span> <span class=n>compressed_aggregate_gradients</span>
</span></span></code></pre></div><p>Our scaling efficiency with halved communication time is:</p><p>$$
\eta = \frac{T_s}{T_s + 0.5 \times T_c} = \frac{234}{234 + 53} \approx 81.5\%
$$</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/orion/gcompression.png alt="Gradient compression results" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Gradient compression results</figcaption></figure><p>&mldr;which is pretty neat! This brings down our training time from 9.1h to 7.7h.</p><h2 id=2-deferring-communication class="relative group">2. Deferring Communication <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#2-deferring-communication aria-label=Anchor>#</a></span></h2><p>Gradient synchronization is required at the end of each batch, and there are only so many samples we can fit in a single forward/backward pass per batch&mldr;</p><p>&mldr;or can we?</p><p>Gradient accumulation is a common technique to emulate large batch sizes on GPUs with limited memory. But this can also be seen as a way of deferring communication. If the maximum batch size supported on a forward/backward pass is 512, which was the case for us here, we could prepare a larger 1024-size batch, and sum over gradients within the GPU with two &ldquo;micro&rdquo; batches.</p><p>The only potential downside of this trick, is if a given model/optimizer does <em>not</em> scale with batch size. This could be the case for small datasets (but then why would you need data parallel?).</p><p>Here is a simple implementation in JAX:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>accumulate_gradient</span><span class=p>(</span><span class=n>value_and_grad_fn</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>params</span><span class=p>:</span> <span class=n>PyTree</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>batch</span><span class=p>:</span> <span class=n>PyTree</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>accum_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>jnp</span><span class=o>.</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>PyTree</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;&#34;&#34;Accumulates gradients over given steps.
</span></span></span><span class=line><span class=cl><span class=s2>  
</span></span></span><span class=line><span class=cl><span class=s2>  Args:
</span></span></span><span class=line><span class=cl><span class=s2>    value_and_grad_fn: Gradient function that does not return aux values.
</span></span></span><span class=line><span class=cl><span class=s2>    params: Parameters, passed as first argument to `value_and_grad_fn`.
</span></span></span><span class=line><span class=cl><span class=s2>    batch: Batch, passed as second argument to `value_and_grad_fn`.
</span></span></span><span class=line><span class=cl><span class=s2>    accum_steps: Number of micro batches to accumulate over. Defaults to 1,
</span></span></span><span class=line><span class=cl><span class=s2>      which means no gradients are accumulated.
</span></span></span><span class=line><span class=cl><span class=s2>  
</span></span></span><span class=line><span class=cl><span class=s2>  Returns:
</span></span></span><span class=line><span class=cl><span class=s2>    Tuple (loss, grads).
</span></span></span><span class=line><span class=cl><span class=s2>  &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>accum_steps</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>bs</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=nb>iter</span><span class=p>(</span><span class=n>jax</span><span class=o>.</span><span class=n>tree</span><span class=o>.</span><span class=n>leaves</span><span class=p>(</span><span class=n>batch</span><span class=p>)))</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>bs</span> <span class=o>%</span> <span class=n>accum_steps</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=sa>f</span><span class=s2>&#34;Invalid accum_steps </span><span class=si>{</span><span class=n>accum_steps</span><span class=si>}</span><span class=s2> for batch size `</span><span class=si>{</span><span class=n>bs</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>microbatch_size</span> <span class=o>=</span> <span class=n>bs</span> <span class=o>//</span> <span class=n>accum_steps</span>
</span></span><span class=line><span class=cl>    <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=s2>&#34;Accumulating with microbatch_size </span><span class=si>%d</span><span class=s2> over </span><span class=si>%d</span><span class=s2> steps.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>microbatch_size</span><span class=p>,</span> <span class=n>accum_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_microbatch</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>i</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=n>jax</span><span class=o>.</span><span class=n>tree</span><span class=o>.</span><span class=n>map</span><span class=p>(</span>
</span></span><span class=line><span class=cl>          <span class=k>lambda</span> <span class=n>t</span><span class=p>:</span> <span class=n>jnp</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>t</span><span class=p>,</span> <span class=p>(</span><span class=n>accum_steps</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>t</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>:]))[</span><span class=n>i</span><span class=p>],</span> <span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Initialize accumulator.</span>
</span></span><span class=line><span class=cl>    <span class=n>l</span><span class=p>,</span> <span class=n>g</span> <span class=o>=</span> <span class=n>value_and_grad_fn</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>get_microbatch</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>accumulate</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>l_and_g</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>l</span><span class=p>,</span> <span class=n>g</span> <span class=o>=</span> <span class=n>l_and_g</span>
</span></span><span class=line><span class=cl>      <span class=n>l_i</span><span class=p>,</span> <span class=n>g_i</span> <span class=o>=</span> <span class=n>value_and_grad_fn</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>get_microbatch</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>i</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      <span class=k>return</span> <span class=p>(</span><span class=n>l</span> <span class=o>+</span> <span class=n>l_i</span><span class=p>,</span> <span class=n>jax</span><span class=o>.</span><span class=n>tree</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=n>jnp</span><span class=o>.</span><span class=n>add</span><span class=p>,</span> <span class=n>g</span><span class=p>,</span> <span class=n>g_i</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Average over accum_steps.</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=p>,</span> <span class=n>grads</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>lax</span><span class=o>.</span><span class=n>fori_loop</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>accum_steps</span><span class=p>,</span> <span class=n>accumulate</span><span class=p>,</span> <span class=p>(</span><span class=n>l</span><span class=p>,</span> <span class=n>g</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>jax</span><span class=o>.</span><span class=n>tree</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span> <span class=o>/</span> <span class=n>accum_steps</span><span class=p>,</span> <span class=p>(</span><span class=n>loss</span><span class=p>,</span> <span class=n>grads</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>value_and_grad_fn</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>batch</span><span class=p>)</span>
</span></span></code></pre></div><p>In theory, you could go all-in with many accumulation steps, such that the communication time as a fraction of total step time tends to zero - giving you an \(\eta \approx 99\%\).</p><p>In our case, we used 2 accumulation steps to match the 4096 batch-size in <a href=https://arxiv.org/abs/1912.11370 target=_blank rel=noreferrer>BiT: BigTransfer</a> paper. Plugging values back into our equation:</p><p>$$
\frac{2 \times T_s}{2 \times T_s + 0.5 \times T_c} = \frac{468}{468 + 53} \approx 89.8\%
$$</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/orion/gaccumulation.png alt="Gradient accumulation results" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Gradient accumulation results</figcaption></figure><p>Ouch, we were SO close to hit our \(90\%\) goal!</p><h2 id=3-faster-communication class="relative group">3. Faster Communication <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#3-faster-communication aria-label=Anchor>#</a></span></h2><p>Ok, no scam going on here. We did <em>not</em> end up buying a faster NIC. Remember that our existing NIC had dual 10G ethernet ports - one of which was running on 1G for networking. We reconfigured all four servers to connect directly to the 10G switch, which in turn was connected to the Internet via a single 1G port.</p><p>On paper, we had 20G bandwidth to/from each node. The question was, did NCCL support multi-NIC? Absolutely it did! I will spare you the details of benchmarking, but these were the two flags we set for NCCL.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>NCCL_SOCKET_IFNAME</span><span class=o>=</span><span class=n>ens803f</span>  <span class=c1># Includes ens803f0 and ens803f1, 10G each.</span>
</span></span><span class=line><span class=cl><span class=n>NCCL_SOCKET_NTHREADS</span><span class=o>=</span><span class=mi>1</span>      <span class=c1># May be different on your setup.</span>
</span></span></code></pre></div><p>With communication speed doubled, we crunch the numbers again:</p><p>$$
\eta = \frac{2 \times T_s}{2 \times T_s + 0.25 \times T_c} = \frac{468}{468 + 27} \approx 94.5\%
$$</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/orion/multinic.png alt="Multi-NIC communication results" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Multi-NIC communication results</figcaption></figure><h2 id=result class="relative group">Result <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#result aria-label=Anchor>#</a></span></h2><p>This cluster achieved a throughput roughly 20% higher than a $16/hr V100 AWS instance. Our team saved ~$120k for close to a year of uptime.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>We actually had an odd number of nodes. I rounded all calculations assuming 4, for it is a nice number for hardware.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md target=_blank rel=noreferrer>NCCL Bandwidth and Throughput Calculation</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://www.johndcook.com/blog/2018/11/15/bfloat16/ target=_blank rel=noreferrer>Comparing <code>bfloat16</code> range and precision to other 16-bit numbers</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=flex><picture class="!mb-0 !mt-0 me-4 w-24 h-auto rounded-full"><img width=646 height=640 class="!mb-0 !mt-0 me-4 w-24 h-auto rounded-full" alt="Karan Shah" loading=lazy decoding=async src=/img/profile.png></picture><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Karan Shah</div><div class="text-sm text-neutral-700 dark:text-neutral-400">Sampling meaning from a non-stationary prior.</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://masterskepticista.github.io/ target=_blank aria-label=Link rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://github.com/MasterSkepticista target=_blank aria-label=Github rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href="https://scholar.google.com/citations?user=vEgcRTkAAAAJ&amp;hl=en" target=_blank aria-label=Google-Scholar rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg height="16" width="16" viewBox="0 0 512 512"><path fill="currentcolor" d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64h0c1.7-3.6 3.6-7.2 5.6-10.7 4.4-7.6 9.4-14.7 15-21.3 27.4-32.6 68.5-53.3 114.4-53.3 33.6.0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0 512 202.7l-94.7 77.1z"/></svg></span></a>
<a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://linkedin.com/in/karan-bhavesh-shah target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://stackoverflow.com/users/9230398/karan-shah target=_blank aria-label=Stack-Overflow rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 384 512"><path fill="currentcolor" d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 3e2zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-2e2v39.7h2e2zm39.7 80H42.7V320h-40v160h359.5V320h-40z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://twitter.com/elevated_quark target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></a></div></div></div></div><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/the-algorithm/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">The Algorithm</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-03-07 00:00:00 +0000 UTC">7 March 2024</time>
</span></span></a></span><span><a class="group flex text-right" href=/posts/detr/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">PyTorch: I’m Fast, JAX: You Call That Fast?</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-08-16 00:00:00 +0000 UTC">16 August 2024</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article></main><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12" id=to-top hidden=true><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div><footer class="py-10 print:hidden"><nav class="pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex list-none flex-col sm:flex-row"><li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a href=/tags/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Tags</span></a></li><li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a href=https://drive.google.com/file/d/1blFQIqhagkHfd3sqsjawCyPCvl0Evrtw/view title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">CV</span></a></li></ul></nav><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">Copyright © 2025 Karan Shah. If you copy this, may your WiFi become unstable.</p></div><div class="flex flex-row items-center"><div class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex h-12 w-12 items-center justify-center dark:hidden" title="Switch to dark appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="hidden h-12 w-12 items-center justify-center dark:flex" title="Switch to light appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://masterskepticista.github.io/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>