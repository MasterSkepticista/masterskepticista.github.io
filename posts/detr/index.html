<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#FFFFFF"><title>PyTorch: I’m Fast, JAX: You Call That Fast? &#183; Home</title>
<meta name=title content="PyTorch: I’m Fast, JAX: You Call That Fast? &#183; Home"><script type=text/javascript src=/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.e0c75b2446b3f8545af40deeeebc2b7899e8a610754fea305617e08710cee268.css integrity="sha256-4MdbJEaz+FRa9A3u7rwreJnophB1T+owVhfghxDO4mg="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.af5d9722112bedac95702865c340bcd6286c4e9b2c15ce26b531ea1fba974cb8.js integrity="sha256-r12XIhEr7ayVcChlw0C81ihsTpssFc4mtTHqH7qXTLg=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        A recipe to train Object Detection Transformers (really) fast.
      
    "><link rel=canonical href=https://masterskepticista.github.io/posts/detr/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://masterskepticista.github.io/posts/detr/"><meta property="og:site_name" content="Home"><meta property="og:title" content="PyTorch: I’m Fast, JAX: You Call That Fast?"><meta property="og:description" content="A recipe to train Object Detection Transformers (really) fast."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-16T00:00:00+00:00"><meta property="article:tag" content="Jax"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="Detr"><meta property="article:tag" content="Object-Detection"><meta property="article:tag" content="Transformers"><meta name=twitter:card content="summary"><meta name=twitter:title content="PyTorch: I’m Fast, JAX: You Call That Fast?"><meta name=twitter:description content="A recipe to train Object Detection Transformers (really) fast."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"PyTorch: I’m Fast, JAX: You Call That Fast?","headline":"PyTorch: I’m Fast, JAX: You Call That Fast?","abstract":"A recipe to train Object Detection Transformers (really) fast.","inLanguage":"en","url":"https:\/\/masterskepticista.github.io\/posts\/detr\/","author":{"@type":"Person","name":"Karan Shah"},"copyrightYear":"2024","dateCreated":"2024-08-16T00:00:00\u002b00:00","datePublished":"2024-08-16T00:00:00\u002b00:00","dateModified":"2024-08-16T00:00:00\u002b00:00","keywords":["jax","pytorch","detr","object-detection","transformers"],"mainEntityOfPage":"true","wordCount":"1445"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://masterskepticista.github.io/","name":"","position":1},{"@type":"ListItem","item":"https://masterskepticista.github.io/posts/","name":"Posts","position":2},{"@type":"ListItem","name":"Py Torch I’m Fast, Jax You Call That Fast?","position":3}]}</script><meta name=author content="Karan Shah"><link href=https://masterskepticista.github.io/ rel=me><link href=https://github.com/MasterSkepticista rel=me><link href="https://scholar.google.com/citations?user=vEgcRTkAAAAJ&amp;hl=en" rel=me><link href=https://linkedin.com/in/karan-bhavesh-shah rel=me><link href=https://stackoverflow.com/users/9230398/karan-shah rel=me><link href=https://twitter.com/elevated_quark rel=me><link type=text/css rel=stylesheet href=/lib/katex/katex.min.68e17230ccd917b97b7a2def38a8108918599d8aa4f580bfb8cce5e13d23e4de43dcaba5f9000553cb2c10d0d1300aabfe5c433a3305ebd752609f0762a63e59.css integrity="sha512-aOFyMMzZF7l7ei3vOKgQiRhZnYqk9YC/uMzl4T0j5N5D3Kul+QAFU8ssENDRMAqr/lxDOjMF69dSYJ8HYqY+WQ=="><script defer src=/lib/katex/katex.min.50f14e69d6a8da7128ae3b63974c544ed377c36d096b5e3750f114e84c89d668b9301d9b0ed3248969aa183aa2e3bc4d2c1e73d5dcb7d462890c45a18d424589.js integrity="sha512-UPFOadao2nEorjtjl0xUTtN3w20Ja143UPEU6EyJ1mi5MB2bDtMkiWmqGDqi47xNLB5z1dy31GKJDEWhjUJFiQ=="></script><script defer src=/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" onload=renderMathInElement(document.body)></script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>Home</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blog</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/tags/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Tags</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">PyTorch: I’m Fast, JAX: You Call That Fast?</h1><div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2024-08-16 00:00:00 +0000 UTC">16 August 2024</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">7 mins</span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 lg:sticky lg:top-10 print:hidden"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#the-bottleneck>The Bottleneck</a></li><li><a href=#baseline>Baseline</a></li><li><a href=#refactor>Refactor</a><ul><li><a href=#1-disable-matching-for-padded-objects>1. Disable Matching for padded objects</a></li><li><a href=#2-mixed-precision-matmuls>2. Mixed Precision MatMuls</a></li><li><a href=#3-parallel-bipartite-matching-on-decoders>3. Parallel Bipartite Matching on Decoders</a></li><li><a href=#4-use-flash-attention>4. Use Flash Attention</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><p>PyTorch is far from being concluded slow. But it is always a fun (and worthwhile) exercise to flex how fast you can <em>really</em> go with compilers if you can lay out a computation in the right way.</p><p>This is my work log of building a Detection Transformer (<a href=https://arxiv.org/abs/2005.12872 target=_blank rel=noreferrer>DETR</a>) training pipeline in JAX. I find this object detection architecture special for many reasons:</p><ol><li>It predicts bounding boxes and class labels directly, instead of generating a gazillion region-proposals and relying on esoteric post-processing techniques.</li><li>It is end-to-end differentiable and parallelizable.</li><li>It fits the &lsquo;spirit of deep learning&rsquo; and borrows wisdom from Rich Sutton&rsquo;s <a href=http://incompleteideas.net/IncIdeas/BitterLesson.html target=_blank rel=noreferrer>Bitter Lesson</a> of AI research.</li></ol><p><a href=https://arxiv.org/abs/2005.12872 target=_blank rel=noreferrer>DETR</a> is one of the well written papers out there, I recommend going through it once.</p><p>However, DETR is slow to train. While there have been successors to DETR that improve algorithmic convergence rates, like <a href=https://arxiv.org/abs/2010.04159 target=_blank rel=noreferrer>Deformable-DETR</a>, or <a href=https://arxiv.org/abs/2108.06152 target=_blank rel=noreferrer>Conditional-DETR</a>, none of these implementations focus on running &rsquo;efficiently&rsquo; on the GPU. There is a great deal of efficiency to be had here, which was the objective of this project. I will walk through the techniques that helped me provide up to \(30\%\) higher GPU utilization against a best-effort optimized <a href=https://github.com/facebookresearch/detr target=_blank rel=noreferrer>PyTorch implementation of DETR</a>.</p><h2 id=the-bottleneck class="relative group">The Bottleneck <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#the-bottleneck aria-label=Anchor>#</a></span></h2><figure class="mx-auto my-0 rounded-md"><img src=https://raw.githubusercontent.com/MasterSkepticista/detr/main/.github/detr.png alt="DETR Architecture" class="mx-auto my-0 rounded-md"><figcaption class=text-center>DETR Architecture</figcaption></figure><p>DETR has three main components: a convolutional backbone (typically a ResNet), a stack of encoder-decoder transformer blocks, and a bipartite matcher. Of the three, bipartite matching (hungarian) algorithm runs on the CPU. In fact, the original DETR implementation calls <a href=https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html target=_blank rel=noreferrer><code>scipy.optimize.linear_sum_assignment</code></a> sequentially, for each input-target pair. This leaves the GPU idle. Part of the gains we will see later, are by reducing this idle time.</p><blockquote><p>Idle GPU is wasted GPU.</p></blockquote><h2 id=baseline class="relative group">Baseline <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#baseline aria-label=Anchor>#</a></span></h2><p>To improve &lsquo;something&rsquo;, we must &lsquo;measure&rsquo; that something. Our bench this time is an 8-A6000 cluster. I made a couple of changes to ensure PyTorch version was &lsquo;as fast as possible&rsquo;. Here is a summary of digressions:</p><ul><li>Use <a href=https://arxiv.org/abs/2205.14135 target=_blank rel=noreferrer>Flash Attention</a> in <code>F.scaled_dot_product_attention</code>.</li><li>Enable use of tensor cores by setting <code>torch.set_float32_matmul_precision(...)</code> to &ldquo;medium&rdquo; (&ldquo;high&rdquo; works just as good).</li><li>Wrap forward pass using <code>torch.autocast</code> to <code>bfloat16</code>.</li></ul><p>With these changes, it took 3 days (2.1 steps/s) to train a 300-epoch baseline on our cluster. I will skip the napkin math, but this is already faster than authors&rsquo; numbers when normalized for per GPU FLOP throughput - notably from use of the new flash attention kernel that Ampere GPUs support.</p><blockquote><p>N.B.: I did try <code>torch.compile</code> with different options on sub-parts of the model/training step, it either ended up giving the same throughput, or failed to compile. So &lsquo;it is what it is&rsquo;.</p></blockquote><h2 id=refactor class="relative group">Refactor <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#refactor aria-label=Anchor>#</a></span></h2><p>I decided to implement DETR in JAX. You can think of JAX as a front-end language to write <a href=https://openxla.org/xla target=_blank rel=noreferrer>XLA</a> optimized programs. XLA is an open-source Machine Learning compiler that optimizes Linear Algebra operations. XLA generally outperforms the superset of all PyTorch optimizations <em>when done right, by a good margin</em>. One downside of working with XLA/JAX is that it is harder to debug <code>jit</code> compiled programs. PyTorch, on the other hand, dispatches CUDA kernels eagerly (except when wrapped with <code>torch.compile</code>), which makes it easiest to debug and work with. But when you consider the cost of few compile minutes over how long production training runs like these typically are, it is worth the tradeoff.</p><blockquote><p>JAX is improving rapidly. I would change my opinion on it being <em>slower</em> rather than <em>harder</em> to debug, as compared to PyTorch.</p></blockquote><p>Luckily a dusty <a href=https://github.com/google-research/scenic/tree/main/scenic/projects/baselines/detr target=_blank rel=noreferrer>re-implementation</a> of DETR in JAX made for a good head-start. But it did not work out-of-the-box due to deprecated JAX and Flax APIs. To get the ball rolling, I made a minimal set of <a href=https://github.com/google-research/scenic/pull/1062 target=_blank rel=noreferrer>changes</a>, without any optimizations.</p><p>Scenic also <a href=https://github.com/google-research/scenic/blob/main/scenic/model_lib/matchers/hungarian_jax.py target=_blank rel=noreferrer>provides</a> GPU and TPU implementations of Hungarian matching. This is already significant work off-the-table.</p><p>This implementation takes 6.5 days to replicate the PyTorch baseline, at nearly 1 step/s. How fast can we go?</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/detr/pt_baseline.png alt class="mx-auto my-0 rounded-md"></figure><p>Now, the optimizations.</p><h3 id=1-disable-matching-for-padded-objects class="relative group">1. Disable Matching for padded objects <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#1-disable-matching-for-padded-objects aria-label=Anchor>#</a></span></h3><hr><p>This is actually a bug-fix rather than an optimization. COCO dataset does not guarantee a fixed number of objects for each image. This means the bipartite matcher would have to map a fixed set of object queries (say 100) to a randomly varying number of target objects for each image, triggering an expensive retrace of the graph.</p><blockquote><p>N.B.: XLA compiler can generate optimized graphs in part because memory allocation/deallocation is predictable, and constant-folding/fusion of operators is simpler when the entire computational graph layout is static. This is the price you pay for performance. You can read more <a href=https://www.tensorflow.org/guide/function target=_blank rel=noreferrer>here</a>.</p></blockquote><p>To prevent retracing, we add &lsquo;padding&rsquo; objects and a boolean mask that allows us to filter dummy objects when computing loss.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Adding padded dimensions</span>
</span></span><span class=line><span class=cl><span class=c1># input_pipeline.py#L145</span>
</span></span><span class=line><span class=cl><span class=n>padded_shapes</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;inputs&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>max_size</span><span class=p>,</span> <span class=n>max_size</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;padding_mask&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>max_size</span><span class=p>,</span> <span class=n>max_size</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;label&#39;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;boxes&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>max_boxes</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;area&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>max_boxes</span><span class=p>,],</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;objects/id&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>max_boxes</span><span class=p>,],</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;is_crowd&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>max_boxes</span><span class=p>,],</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;labels&#39;</span><span class=p>:</span> <span class=p>[</span><span class=n>max_boxes</span><span class=p>,],</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;image/id&#39;</span><span class=p>:</span> <span class=p>[],</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;size&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>2</span><span class=p>,],</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;orig_size&#39;</span><span class=p>:</span> <span class=p>[</span><span class=mi>2</span><span class=p>,],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>But this still computes bipartite matching on <code>padded</code> objects. We can remove
constants from the <code>cost</code> matrix as they do not affect the final matching.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-patch data-lang=patch><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=gd>-- cost = cost * mask + (1.0 - mask) * cost_upper_bound
</span></span></span><span class=line><span class=cl><span class=gd></span><span class=gi>++ cost = cost * mask
</span></span></span></code></pre></div><p>With this bug-fix, we are now 40% faster, i.e. \(1.4\) steps/s. It now takes 4.7 days to train the baseline.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/detr/disable_padded.png alt="Disable Matching for padded objects" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Disable Matching for padded objects</figcaption></figure><h3 id=2-mixed-precision-matmuls class="relative group">2. Mixed Precision MatMuls <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#2-mixed-precision-matmuls aria-label=Anchor>#</a></span></h3><p>Yes, there are no &lsquo;free-lunches&rsquo;, but I think we can make a strong case for the invention of <code>bfloat16</code> data type.
We migrate <code>float32</code> matmuls to <code>bfloat16</code>, without any loss in final AP scores. This is what we did in the PyTorch baseline.
In <code>flax</code>, this is the same as supplying <code>dtype=jnp.bfloat16</code> on supported modules.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Example conversion.</span>
</span></span><span class=line><span class=cl><span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>jnp</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>dense</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=o>...</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>jnp</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span></code></pre></div><p>This gets us above \(2.1\) steps/s. We now have performance parity with PyTorch, with 3.1 days taken to train the baseline!</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/detr/mixed_prec.png alt class="mx-auto my-0 rounded-md"></figure><p>Huh! We should&rsquo;ve called it a day&mldr; but let&rsquo;s keep going.</p><h3 id=3-parallel-bipartite-matching-on-decoders class="relative group">3. Parallel Bipartite Matching on Decoders <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#3-parallel-bipartite-matching-on-decoders aria-label=Anchor>#</a></span></h3><p>To achieve a high overall \(\text{mAP}\) score, DETR authors propose computing loss over each decoder output. DETR uses a sequential stack of 6 decoders, each emitting bounding-box and classifier predictions for 100 object queries.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># models/detr_base_model.py#L377</span>
</span></span><span class=line><span class=cl><span class=c1># Computing matchings for each decoder head (auxiliary predictions)</span>
</span></span><span class=line><span class=cl><span class=c1># outputs = {</span>
</span></span><span class=line><span class=cl><span class=c1>#   &#34;pred_logits&#34;: ndarray, </span>
</span></span><span class=line><span class=cl><span class=c1>#   &#34;pred_boxes&#34;: ndarray,</span>
</span></span><span class=line><span class=cl><span class=c1>#   &#34;aux_outputs&#34;: [</span>
</span></span><span class=line><span class=cl><span class=c1>#     {&#34;pred_logits&#34;: ndarray, &#34;pred_boxes&#34;: ndarray},</span>
</span></span><span class=line><span class=cl><span class=c1>#     {&#34;pred_logits&#34;: ndarray, &#34;pred_boxes&#34;: ndarray},</span>
</span></span><span class=line><span class=cl><span class=c1>#     ...</span>
</span></span><span class=line><span class=cl><span class=c1>#   ]</span>
</span></span><span class=line><span class=cl><span class=c1># }</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>matches</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>cost</span><span class=p>,</span> <span class=n>n_cols</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_cost_matrix</span><span class=p>(</span><span class=n>outputs</span><span class=p>,</span> <span class=n>batch</span><span class=p>[</span><span class=s1>&#39;label&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>matches</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>matcher</span><span class=p>(</span><span class=n>cost</span><span class=p>,</span> <span class=n>n_cols</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=s1>&#39;aux_outputs&#39;</span> <span class=ow>in</span> <span class=n>outputs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>matches</span> <span class=o>=</span> <span class=p>[</span><span class=n>matches</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>aux_pred</span> <span class=ow>in</span> <span class=n>outputs</span><span class=p>[</span><span class=s1>&#39;aux_outputs&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>      <span class=n>cost</span><span class=p>,</span> <span class=n>n_cols</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_cost_matrix</span><span class=p>(</span><span class=n>aux_pred</span><span class=p>,</span> <span class=n>batch</span><span class=p>[</span><span class=s1>&#39;label&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>matches</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>matcher</span><span class=p>(</span><span class=n>cost</span><span class=p>,</span> <span class=n>n_cols</span><span class=p>))</span>
</span></span></code></pre></div><p>Computing optimal matchings on these decoder outputs can actually be done in parallel using <code>vmap</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># models/detr_base_model.py#L377</span>
</span></span><span class=line><span class=cl><span class=c1># After vectorization</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>matches</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>predictions</span> <span class=o>=</span> <span class=p>[{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;pred_logits&#34;</span><span class=p>:</span> <span class=n>outputs</span><span class=p>[</span><span class=s2>&#34;pred_logits&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;pred_boxes&#34;</span><span class=p>:</span> <span class=n>outputs</span><span class=p>[</span><span class=s2>&#34;pred_boxes&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=p>}]</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=s1>&#39;aux_outputs&#39;</span> <span class=ow>in</span> <span class=n>outputs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=n>outputs</span><span class=p>[</span><span class=s2>&#34;aux_outputs&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>def</span> <span class=nf>_compute_matches</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>targets</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>cost</span><span class=p>,</span> <span class=n>n_cols</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_cost_matrix</span><span class=p>(</span><span class=n>predictions</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>matcher</span><span class=p>(</span><span class=n>cost</span><span class=p>,</span> <span class=n>n_cols</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Stack list of pytrees.</span>
</span></span><span class=line><span class=cl>  <span class=n>predictions</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>tree</span><span class=o>.</span><span class=n>map</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=k>lambda</span> <span class=o>*</span><span class=n>args</span><span class=p>:</span> <span class=n>jnp</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>args</span><span class=p>),</span> <span class=o>*</span><span class=n>predictions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Compute matches in parallel for all outputs.</span>
</span></span><span class=line><span class=cl>  <span class=n>matches</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>vmap</span><span class=p>(</span><span class=n>_compute_matches</span><span class=p>,</span> <span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=kc>None</span><span class=p>))(</span>
</span></span><span class=line><span class=cl>    <span class=n>predictions</span><span class=p>,</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&#34;label&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>matches</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>matches</span><span class=p>)</span>
</span></span></code></pre></div><p>With this change, we are now stepping <strong>10% faster</strong> than PyTorch, at \(2.4\) steps/s, i.e. 2.7 days to train.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/detr/parallel_match.png alt class="mx-auto my-0 rounded-md"></figure><h3 id=4-use-flash-attention class="relative group">4. Use Flash Attention <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#4-use-flash-attention aria-label=Anchor>#</a></span></h3><p>XLA did not use flash attention kernel all along. It was added only recently through <a href=https://github.com/google/jax/pull/21371 target=_blank rel=noreferrer><code>jax.nn.dot_product_attention</code></a> for Ampere and later architectures. Perhaps future XLA versions might automatically recognize a dot-product attention signature during <code>jit</code>, without us having to explicitly call via SDPA API. But that is not the case today, so we will make-do with this custom function call.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># models/detr.py#L261</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>x</span> <span class=o>=</span> <span class=n>jax</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>dot_product_attention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span> <span class=n>implementation</span><span class=o>=</span><span class=s2>&#34;cudnn&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>x</span> <span class=o>=</span> <span class=n>attention_layers</span><span class=o>.</span><span class=n>dot_product_attention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=n>query</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>key</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>value</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>dropout_rate</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout_rate</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>broadcast_dropout</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>broadcast_dropout</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>dropout_rng</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>make_rng</span><span class=p>(</span><span class=s1>&#39;dropout&#39;</span><span class=p>)</span> <span class=k>if</span> <span class=n>train</span> <span class=k>else</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>deterministic</span><span class=o>=</span><span class=ow>not</span> <span class=n>train</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=n>capture_attention_weights</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span></code></pre></div><blockquote><p>Note: As of writing SDPA API does not support attention dropout. This is because JAX and cuDNN use different PRNG implementations. Once <code>dropout</code> makes its way to SDPA API, we can set flash attention to be our default.</p></blockquote><p>For now, let us be content with the <em>potential</em> speedup. We are now at \(3.0\) steps/s, <strong>33% faster</strong> than PyTorch, taking 2 days to train.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/detr/flash_attn.png alt class="mx-auto my-0 rounded-md"></figure><h2 id=summary class="relative group">Summary <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#summary aria-label=Anchor>#</a></span></h2><p>Further gains are possible by replacing exact matching with an approximate matching. It may be a good reason to do so - just like how minibatch SGD is random by its very nature. It is arguably its strong suit on its nice convergence properties.</p><p>Why should a matching algorithm be exact, if we are spending ~0.5M steps to converge anyway? Are there gains to be had by having an &lsquo;approximate&rsquo; matching? Yes, and one way to go about it is using a regularized solver like Sinkhorn algorithm. But that&rsquo;s for another day.</p><p>You can find the code for DETR with all above optimizations <a href=https://github.com/masterskepticista/detr target=_blank rel=noreferrer>here</a>. Update: It also supports Sinkhorn algorithm now!</p></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=flex><picture class="!mb-0 !mt-0 me-4 w-24 h-auto rounded-full"><img width=646 height=640 class="!mb-0 !mt-0 me-4 w-24 h-auto rounded-full" alt="Karan Shah" loading=lazy decoding=async src=/img/profile.png></picture><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Karan Shah</div><div class="text-sm text-neutral-700 dark:text-neutral-400">Sampling meaning from a non-stationary prior.</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://masterskepticista.github.io/ target=_blank aria-label=Link rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://github.com/MasterSkepticista target=_blank aria-label=Github rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href="https://scholar.google.com/citations?user=vEgcRTkAAAAJ&amp;hl=en" target=_blank aria-label=Google-Scholar rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg height="16" width="16" viewBox="0 0 512 512"><path fill="currentcolor" d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64h0c1.7-3.6 3.6-7.2 5.6-10.7 4.4-7.6 9.4-14.7 15-21.3 27.4-32.6 68.5-53.3 114.4-53.3 33.6.0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0 512 202.7l-94.7 77.1z"/></svg></span></a>
<a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://linkedin.com/in/karan-bhavesh-shah target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://stackoverflow.com/users/9230398/karan-shah target=_blank aria-label=Stack-Overflow rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 384 512"><path fill="currentcolor" d="M290.7 311 95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 3e2zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-2e2v39.7h2e2zm39.7 80H42.7V320h-40v160h359.5V320h-40z"/></svg>
</span></a><a class="px-1 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://twitter.com/elevated_quark target=_blank aria-label=X-Twitter rel="me noopener noreferrer"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></span></a></div></div></div></div><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/orion/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Data Parallelism using standard Ethernet</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-08-05 00:00:00 +0000 UTC">5 August 2024</time>
</span></span></a></span><span><a class="group flex text-right" href=/posts/graph-search/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Graph Search</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-08-27 00:00:00 +0000 UTC">27 August 2024</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article></main><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12" id=to-top hidden=true><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">Copyright © 2025 Karan Shah. If you copy this, may your WiFi become unstable.</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://github.com/jpanther/congo target=_blank rel="noopener noreferrer">Congo</a></p></div><div class="flex flex-row items-center"></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://masterskepticista.github.io/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>