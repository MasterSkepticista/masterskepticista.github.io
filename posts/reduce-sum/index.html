<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#FFFFFF"><title>Embarrassingly Parallel Reduction in CUDA &#183; MasterSkepticista</title>
<meta name=title content="Embarrassingly Parallel Reduction in CUDA &#183; MasterSkepticista"><script type=text/javascript src=/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.eaf5efab3f4188a116dda7ffc5498d61662733eaf22fa6a2fcb554efa84c23c8.css integrity="sha256-6vXvqz9BiKEW3af/xUmNYWYnM+ryL6ai/LVU76hMI8g="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.bb487ad6073790cb02c354ee8c5f8822c42c5513e10bf7a86bbe8f82118cd1fc.js integrity="sha256-u0h61gc3kMsCw1TujF+IIsQsVRPhC/eoa76PghGM0fw=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        A step-by-step guide on turning simple math into a flex.
      
    "><link rel=canonical href=https://masterskepticista.github.io/posts/reduce-sum/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://masterskepticista.github.io/posts/reduce-sum/"><meta property="og:site_name" content="MasterSkepticista"><meta property="og:title" content="Embarrassingly Parallel Reduction in CUDA"><meta property="og:description" content="A step-by-step guide on turning simple math into a flex."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-19T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-19T00:00:00+00:00"><meta property="article:tag" content="Cuda"><meta name=twitter:card content="summary"><meta name=twitter:title content="Embarrassingly Parallel Reduction in CUDA"><meta name=twitter:description content="A step-by-step guide on turning simple math into a flex."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Embarrassingly Parallel Reduction in CUDA","headline":"Embarrassingly Parallel Reduction in CUDA","abstract":"A step-by-step guide on turning simple math into a flex.","inLanguage":"en","url":"https:\/\/masterskepticista.github.io\/posts\/reduce-sum\/","author":{"@type":"Person","name":"Karan Shah"},"copyrightYear":"2025","dateCreated":"2025-02-19T00:00:00\u002b00:00","datePublished":"2025-02-19T00:00:00\u002b00:00","dateModified":"2025-02-19T00:00:00\u002b00:00","keywords":["cuda"],"mainEntityOfPage":"true","wordCount":"1980"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://masterskepticista.github.io/","name":"","position":1},{"@type":"ListItem","item":"https://masterskepticista.github.io/posts/","name":"Posts","position":2},{"@type":"ListItem","name":"Embarrassingly Parallel Reduction in Cuda","position":3}]}</script><meta name=author content="Karan Shah"><link href=https://masterskepticista.github.io/ rel=me><link href=https://github.com/MasterSkepticista rel=me><link href="https://scholar.google.com/citations?user=vEgcRTkAAAAJ&amp;hl=en" rel=me><link href=https://linkedin.com/in/karan-bhavesh-shah rel=me><link href=https://stackoverflow.com/users/9230398/karan-shah rel=me><link href=https://twitter.com/elevated_quark rel=me><link type=text/css rel=stylesheet href=/lib/katex/katex.min.68e17230ccd917b97b7a2def38a8108918599d8aa4f580bfb8cce5e13d23e4de43dcaba5f9000553cb2c10d0d1300aabfe5c433a3305ebd752609f0762a63e59.css integrity="sha512-aOFyMMzZF7l7ei3vOKgQiRhZnYqk9YC/uMzl4T0j5N5D3Kul+QAFU8ssENDRMAqr/lxDOjMF69dSYJ8HYqY+WQ=="><script defer src=/lib/katex/katex.min.50f14e69d6a8da7128ae3b63974c544ed377c36d096b5e3750f114e84c89d668b9301d9b0ed3248969aa183aa2e3bc4d2c1e73d5dcb7d462890c45a18d424589.js integrity="sha512-UPFOadao2nEorjtjl0xUTtN3w20Ja143UPEU6EyJ1mi5MB2bDtMkiWmqGDqi47xNLB5z1dy31GKJDEWhjUJFiQ=="></script><script defer src=/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF+NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w==" onload=renderMathInElement(document.body)></script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden"><nav class="flex items-start justify-between sm:items-center"><div class="z-40 flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>MasterSkepticista</a></div><label id=menu-button for=menu-controller class="block sm:hidden"><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper class="invisible fixed inset-0 z-30 m-auto h-full w-full cursor-default overflow-auto bg-neutral-100/50 opacity-0 backdrop-blur-sm transition-opacity dark:bg-neutral-900/50"><ul class="mx-auto flex w-full max-w-7xl list-none flex-col overflow-visible px-6 py-6 text-end sm:px-14 sm:py-10 sm:pt-10 md:px-24 lg:px-32"><li class=mb-1><span class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class="group mb-1"><a href=/posts/ title onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blog</span></a></li><li class="group mb-1"><a href=/code/ title onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Code</span></a></li><li class="group mb-1"><a href=/talks/ title onclick=close_menu()><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Talks</span></a></li><li class="group mb-1"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></div></label><ul class="hidden list-none flex-row text-end sm:flex"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/posts/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blog</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/code/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Code</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><a href=/talks/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Talks</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0"><button id=search-button-2 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="hidden inline"><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/></a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/posts/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class="hidden inline"><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/posts/reduce-sum/>Embarrassingly Parallel Reduction in CUDA</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Embarrassingly Parallel Reduction in CUDA</h1><div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2025-02-19 00:00:00 +0000 UTC">19 February 2025</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">10 mins</span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 lg:sticky lg:top-10 print:hidden"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#roofline>Roofline</a></li><li><a href=#complexity-analysis>Complexity Analysis</a></li><li><a href=#baseline>Baseline</a></li><li><a href=#kernel-1-vector-loads>Kernel 1: Vector Loads</a></li><li><a href=#kernel-2-tree-reduction>Kernel 2: Tree Reduction</a></li><li><a href=#kernel-3-non-divergent-threads>Kernel 3: Non-divergent Threads</a><ul><li><a href=#bank-conflictsbank-conflicts>Bank Conflicts</a></li></ul></li><li><a href=#kernel-4-sequential-addressing>Kernel 4: Sequential Addressing</a></li><li><a href=#kernel-5-reduce-on-first-load>Kernel 5: Reduce on First Load</a></li><li><a href=#kernel-6-warp-unrolling>Kernel 6: Warp Unrolling</a></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><p>Reduce operations are common in HPC applications. Put simply, a reduce operation combines all elements of an array into a single value through either <code>sum</code>, <code>min</code>, <code>max</code>, <code>product</code>, etc.</p><p>Reduce operations are embarrasingly parallel<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, which makes them a great candidate to be run on GPUs.</p><p>This post will walk through a series of optimizations to iteratively obtain maximum device throughput.</p><p>Code is available on <a href=https://github.com/MasterSkepticista/parallel_reductions_cuda target=_blank rel=noreferrer>GitHub</a>.</p><div style=margin:auto;width:auto><table style=font-size:.9em><thead><tr><th>#</th><th>Kernel</th><th>Bandwidth (GB/s)</th><th>Relative to <code>jnp.sum</code></th></tr></thead><tbody><tr><td>1</td><td>Vector Loads</td><td>9.9</td><td>1.1%</td></tr><tr><td>2</td><td>Tree Reduction</td><td>223</td><td>24.7%</td></tr><tr><td>3</td><td>Non-divergent Threads</td><td>317</td><td>36.3%</td></tr><tr><td>4</td><td>Sequential Addressing</td><td>331</td><td>38.0%</td></tr><tr><td>5</td><td>Reduce on First Load</td><td>618</td><td>70.9%</td></tr><tr><td>6</td><td>Warp Unrolling</td><td>859</td><td>98.6%</td></tr><tr><td>0</td><td><code>jnp.sum</code> reference</td><td>871</td><td>100%</td></tr></tbody></table></div><h2 id=roofline class="relative group">Roofline <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#roofline aria-label=Anchor>#</a></span></h2><p>Our bench problem is to compute the sum of elements of a vector with \( N = 2^{25} \) floats. A reduce operation reads each element of the array at least once. And because this is a <code>sum</code> operation, it will also do a total of \(N-1\) adds. First, we will lower-bound our runtime based on peak compute and memory throughput values of an RTX-3090<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>:</p><ul><li>\(N\) <code>float32</code> reads at 936 GB/s = 0.136 ms</li><li>\(N-1\) <code>float32</code> adds at 35.6 TFLOPS = 0.0036 ms</li></ul><p>As we can see, reduction operations have very low arithmetic intensity, they are memory-bound.</p><p>The theoretical minimum time for this operation is 0.136 + 0.0036 = 0.1396 ms. Since CUDA does not provide a built-in <code>reduce_sum</code> primitive, we will use <code>jax.numpy.sum</code> as a reference. <code>jnp.sum</code> completes in 0.15 ms, achieving <strong>871 GB/s</strong> effective bandwidth.</p><h2 id=complexity-analysis class="relative group">Complexity Analysis <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#complexity-analysis aria-label=Anchor>#</a></span></h2><p><a href=https://stanford.edu/~rezab/dao/notes/lecture01/cme323_lec1.pdf target=_blank rel=noreferrer>Brent&rsquo;s theorem</a> is how we compare efficiency across different parallel algorithms. For a parallel algorithm, we can calculate:</p><ul><li>\(W\): work, the total number of operations if run serially.</li><li>\(S\): step complexity, or the serialization cost when certain operations cannot be parallelized.</li><li>\(P\): number of parallel processors.</li></ul><p>Given this, time complexity with \(P\) parallel processors is defined as:</p><p>$$
T_p \le \frac{W}{P} + S
$$</p><p>We can derive a couple of interesting bounds which will be helpful later:</p><ul><li>\(T_\infty = S\): Infinite parallelism boils down to the time taken in sequential operations.</li><li>\(T_1 = W\): Single processor time is when the total work is performed sequentially.</li><li>\(T_1 / T_p\): Speedup when using \(P\) processors.</li><li>\(W / S\): Parallelism achievable in the algorithm.</li></ul><p>For the <code>reduce_sum</code> operation, work complexity \(W = N_{reads} + (N-1)_{adds}\) is \(O(N)\).</p><h2 id=baseline class="relative group">Baseline <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#baseline aria-label=Anchor>#</a></span></h2><p>Jensen would hate me for using GPUs to sum \(N\) elements using atomic operations. But this &ldquo;serial&rdquo; operation serves as a baseline for all parallelism we will achieve later.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>reduce_sum_kernel1</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>out</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span><span class=n>arr</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>idx</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>atomicAdd</span><span class=p>(</span><span class=n>out</span><span class=p>,</span> <span class=n>arr</span><span class=p>[</span><span class=n>idx</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This kernel achieves 2.47 GB/s effective bandwidth.</p><h2 id=kernel-1-vector-loads class="relative group">Kernel 1: Vector Loads <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-1-vector-loads aria-label=Anchor>#</a></span></h2><p>We will start with a naive optimization. We let each thread compute a portion of the array sum in parallel, and accumulate the partial sums to the output. Since our GPU supports 128-byte load/stores, we will use the <code>float4</code> vector data type.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/reduce-sum/1_float4_sum.png alt="Vector Loads" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Vector Loads</figcaption></figure><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>reduce_sum_kernel1</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>out</span><span class=p>,</span> <span class=k>const</span> <span class=n>float4</span> <span class=o>*</span><span class=n>arr</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>idx</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>float4</span> <span class=n>val</span> <span class=o>=</span> <span class=n>arr</span><span class=p>[</span><span class=n>idx</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>atomicAdd</span><span class=p>(</span><span class=n>out</span><span class=p>,</span> <span class=n>val</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>val</span><span class=p>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>val</span><span class=p>.</span><span class=n>z</span> <span class=o>+</span> <span class=n>val</span><span class=p>.</span><span class=n>w</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Technicaly, this kernel could even saturate the memory bus using \(k\)-wide vector loads and \(k\) sums per thread. We achieve 9.9 GB/s effective bandwidth with \(k=4\): a \(4\times\) throughput.</p><p>Let&rsquo;s not be fooled here. This is actually not a parallel algorithm.</p><p>Complexity analysis:</p><ul><li><p>Work complexity \(W\) of this kernel is \(O(N)\) - all elements of the array are accessed and summed once. This is the minimum work required even for a serial algorithm. Therefore, this kernel is <strong>work efficient</strong>.</p></li><li><p>Step complexity \(S\) for this kernel is \(O(N/4) \approx O(N)\) - as all \(N/4\) threads must wait for the <code>atomicAdd</code> to complete. This kernel is <strong>not</strong> step efficient.</p></li><li><p>Parallelism ratio \(\frac{W}{S} = \frac{O(N)}{O(N)} = 1\). This means the kernel does not scale with processors. In other words: even if we had infinite processors with infinite memory bandwidth, this kernel behaves serially under <code>atomicAdd</code> operations.</p></li></ul><p>An algorithm is parallelizable if \(\frac{W}{S} > 1\). Can we compute sum in less than \(N\) steps?</p><h2 id=kernel-2-tree-reduction class="relative group">Kernel 2: Tree Reduction <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-2-tree-reduction aria-label=Anchor>#</a></span></h2><p>The problem with the previous kernel is that it was not step efficient. It takes \(O(N)\) steps to sum over the array. Using a binary tree reduction, we can sum the array in \(\log N\) steps. Note that we still do \(N-1\) adds and \(N\) reads. Therefore this approach remains work efficient.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/reduce-sum/tree_sum.png alt="Tree Reduction" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Tree Reduction</figcaption></figure><p>In CUDA, threads are grouped as &ldquo;blocks&rdquo;. We will divide the array into blocks of certain size, which is a tunable parameter. Each block will sum its elements in parallel, and then the partial sums will be accumulated using atomics. For simplicity, I will depict the reduction process for a <code>block_size</code> of 8.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/reduce-sum/blocks.png alt=Blocks class="mx-auto my-0 rounded-md"><figcaption class=text-center>Blocks</figcaption></figure><p>We load subarrays of size <code>block_size</code> into shared memory. We will use this buffer to store the partial sums.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// a buffer that can hold 8 floats
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>extern</span> <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>buffer</span><span class=p>[];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=cm>/**
</span></span></span><span class=line><span class=cl><span class=cm> * blockDim.x = 8, number of threads per block
</span></span></span><span class=line><span class=cl><span class=cm> * blockIdx.x = 0 or 1, id of the block
</span></span></span><span class=line><span class=cl><span class=cm> * threadIdx.x = 0...7, id of the thread within the block
</span></span></span><span class=line><span class=cl><span class=cm> */</span>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// thread idx within the block, 0...7
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Load a tile
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>=</span> <span class=n>arr</span><span class=p>[</span><span class=n>idx</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Don&#39;t proceed until all threads have loaded their values
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>__syncthreads</span><span class=p>();</span>
</span></span></code></pre></div><figure class="mx-auto my-0 rounded-md"><img src=/posts/reduce-sum/2_divergent.png alt="Divergent Threads" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Divergent Threads</figcaption></figure><p>We perform the reduction once the buffer is populated. GPU invokes all blocks in parallel. Each block performs a tree reduction on the buffer. The process is as follows:</p><ol><li>At the first step, consecutive elements are summed by <code>tid=[0, 2, 4, 6]</code>.</li><li>At the second step, every second element gets summed by every second thread <code>tid=[0, 4]</code>.</li><li>This process continues until the final sum is stored in <code>buffer[0]</code>.</li></ol><p>After \(\log N\) reductions, each block performs an <code>atomicAdd</code> to the output. Note that this kernel <strong>always</strong> performs as many <code>atomicAdd</code> operations as there are number of blocks executing in parallel. Unlike our vector-load kernel, the number of atomic operations here is not dependent on the array size.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>reduce_sum_kernel1</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>out</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span><span class=n>arr</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>extern</span> <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>buffer</span><span class=p>[];</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>idx</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>idx</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>=</span> <span class=n>arr</span><span class=p>[</span><span class=n>idx</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>s</span> <span class=o>=</span> <span class=mi>1</span><span class=p>;</span> <span class=n>s</span> <span class=o>&lt;</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>;</span> <span class=n>s</span> <span class=o>*=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>%</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>s</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=n>s</span><span class=p>];</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>atomicAdd</span><span class=p>(</span><span class=n>out</span><span class=p>,</span> <span class=n>buffer</span><span class=p>[</span><span class=mi>0</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This kernel achieves <strong>223 GB/s</strong> effective bandwidth, a \(23\times\) improvement.</p><p>Complexity analysis:</p><ul><li>Work complexity \(W\) of this kernel is \(O(N)\). Therefore, this kernel is <strong>work efficient</strong>.</li><li>Step complexity \(S\) for this kernel is \(O(\log N)\). Therefore, this kernel is <strong>step efficient</strong>.</li><li>Parallelism ratio \(\frac{W}{S} = \frac{O(N)}{O(\log N)} = O(N/\log N)\) is greater than 1. This is our first truly parallel algorithm.</li></ul><h2 id=kernel-3-non-divergent-threads class="relative group">Kernel 3: Non-divergent Threads <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-3-non-divergent-threads aria-label=Anchor>#</a></span></h2><p>Now that we have an algorithm that is both work and step efficient, all further improvements will be a result of specializing memory and computation patterns for the actual hardware.</p><p>The previous kernel suffers from two issues:</p><ol><li><p><strong>Warp divergence:</strong> Odd numbered threads remain unused. On CUDA devices, threads within a block are organized in groups of <code>32</code>, called warps. When threads of a warp &ldquo;branch&rdquo;, the warp is said to be divergent. Compiler serializes these branches.</p></li><li><p><strong>Modulo arithmetic:</strong> At each step, half the active threads enter the <code>if</code> block, perform expensive arithmetic while not participating in the computation.</p></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Only evenly strided threads are active, plus % is wasteful.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>%</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>s</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=n>s</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We eliminate <code>%</code> operation and compute an <code>index</code> such that only consecutive threads in a block are active.
The inner loop now becomes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>int</span> <span class=n>index</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>s</span> <span class=o>*</span> <span class=n>tid</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>index</span> <span class=o>&lt;</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>buffer</span><span class=p>[</span><span class=n>index</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>index</span> <span class=o>+</span> <span class=n>s</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><figure class="mx-auto my-0 rounded-md"><img src=/posts/reduce-sum/3_nondivergent.png alt="Non-divergent Threads" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Non-divergent Threads</figcaption></figure><p>With this change, our kernel achieves <strong>317 GB/s</strong> effective bandwidth, a <strong>42%</strong> improvement over the previous kernel.</p><hr><h3 id=bank-conflictsbank-conflicts class="relative group">Bank Conflicts<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#bank-conflictsbank-conflicts aria-label=Anchor>#</a></span></h3><p>On CUDA devices, shared memory is divided into groups of <code>32</code> banks, assuming each index holds <code>4</code>-byte wide memory (called a <code>word</code>). Bank ID relates to the index of memory being accessed as follows:</p><p>$$
\text{bank} = \text{index } % \text{ 32}
$$</p><p>If different threads access different banks in parallel, shared memory can serve all those requests with no penalty. However, if two threads access indices from the same bank at the same time, the memory controller serializes these requests. These are called bank conflicts. Below is an example of a two-way bank conflict when different threads wrap around the same bank index: <code>buffer[threadIdx.x * 2]</code></p><figure class="mx-auto my-0 rounded-md"><img src=/posts/reduce-sum/two_way_conflict.png alt="Two-way Bank Conflict" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Two-way Bank Conflict</figcaption></figure><p>For instance, with our non-divergent threads kernel:</p><ul><li>At stride <code>s=1</code>, <code>tid=[0,16]</code> access <code>index=[0,32]</code> and <code>index[1,33]</code> which belong to the same bank.</li><li>At stride <code>s=2</code>, <code>tid=[0,8]</code> access <code>index=[0,32]</code> and <code>index[2,34]</code> which belong to the same bank.
and so on.</li></ul><p>In summary, when different threads start accessing addresses that wrap around the bank index (due to modulo 32 behavior), it may lead to 2-way, 4-way, or even higher-degree conflicts. We will see how to eliminate bank conflicts in the next kernel.</p><h2 id=kernel-4-sequential-addressing class="relative group">Kernel 4: Sequential Addressing <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-4-sequential-addressing aria-label=Anchor>#</a></span></h2><p>There is one neat trick up CUDA&rsquo;s sleeves. It is not a bank conflict if the same thread accesses multiple addresses within the same bank. Looking at the conflict example, we want <code>tid=0</code> to access <code>index=[0,32]</code>, <code>tid=1</code> to access <code>index=[1,33]</code> and so on. To do so, we invert the stride calculation.</p><figure class="mx-auto my-0 rounded-md"><img src=/posts/reduce-sum/4_sequential.png alt="Sequential Addressing" class="mx-auto my-0 rounded-md"><figcaption class=text-center>Sequential Addressing</figcaption></figure><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// halve stride on each step
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>s</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>s</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>;</span> <span class=n>s</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// bounds checking
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>&lt;</span> <span class=n>s</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=n>s</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This kernel achieves <strong>335 GB/s</strong> effective bandwidth. While this change might not seem as impressive in terms of speedup, sequential addressing and elimination of bank conflicts is the foundation for our next banger.</p><h2 id=kernel-5-reduce-on-first-load class="relative group">Kernel 5: Reduce on First Load <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-5-reduce-on-first-load aria-label=Anchor>#</a></span></h2><p>Half the threads in each block do not perform any computation during the first reduction step. We can halve the total blocks and let each thread perform first level of reduction while loading elements in the buffer.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>=</span> <span class=n>arr</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=o>+</span> <span class=n>arr</span><span class=p>[</span><span class=n>idx</span> <span class=o>+</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span><span class=p>];</span>
</span></span></code></pre></div><figure class="mx-auto my-0 rounded-md"><img src=/posts/reduce-sum/5_noidle_threads.png alt="No Idle Threads" class="mx-auto my-0 rounded-md"><figcaption class=text-center>No Idle Threads</figcaption></figure><p>This kernel achieves <strong>618 GB/s</strong> effective bandwidth, over <strong>1.86</strong> \(\times\) faster.</p><h2 id=kernel-6-warp-unrolling class="relative group">Kernel 6: Warp Unrolling <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#kernel-6-warp-unrolling aria-label=Anchor>#</a></span></h2><p>Sequential addressing has a key feature: threads that finish execution become idle for the rest of the program. When number of active threads become less than 32, all these threads are part of the same &ldquo;warp&rdquo; in their respective blocks. Threads that are part of the same warp do not need thread synchronization calls or bounds checking. This reduces instruction overhead.</p><p>By unrolling the reduction for last 32 elements, we eliminate useless work in all warps of all blocks.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Reduce synchronously across blocks until we have 32 threads left.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>s</span> <span class=o>=</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>/</span> <span class=mi>2</span><span class=p>;</span> <span class=n>s</span> <span class=o>&gt;</span> <span class=mi>32</span><span class=p>;</span> <span class=n>s</span> <span class=o>/=</span> <span class=mi>2</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>&lt;</span> <span class=n>s</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=n>s</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// Unroll and reduce for the last 32 threads.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>if</span> <span class=p>(</span><span class=n>tid</span> <span class=o>&lt;</span> <span class=mi>32</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>warpReduce</span><span class=p>(</span><span class=n>buffer</span><span class=p>,</span> <span class=n>tid</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We define a device function to unroll the warp and perform reductions.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__device__</span> <span class=kt>void</span> <span class=nf>warpReduce</span><span class=p>(</span><span class=k>volatile</span> <span class=kt>float</span> <span class=o>*</span><span class=n>buffer</span><span class=p>,</span> <span class=kt>int</span> <span class=n>tid</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=mi>32</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=mi>16</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=mi>8</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=mi>4</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>+=</span> <span class=n>buffer</span><span class=p>[</span><span class=n>tid</span> <span class=o>+</span> <span class=mi>1</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We are now at <strong>859 GB/s</strong> effective bandwidth, within ~1% of <code>jax.numpy.sum</code>.</p><p>Who doesn&rsquo;t like speed?</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Associativity [<code>(a + b) + c = a + (b + c)</code>] is a trait of highly parallel algorithms: since grouping of operations does not affect the result, algorithms can compute partial results independently.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf target=_blank rel=noreferrer>NVIDIA Ampere Architecture Datasheet</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://github.com/Kobzol/hardware-effects-gpu/tree/master/bank-conflicts target=_blank rel=noreferrer>This</a> repo on demonstrating hardware effects on GPU is useful.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span></span><span><a class="group flex text-right" href=/posts/starship-score/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">'Notes' of a Launch</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-12-21 00:00:00 +0000 UTC">21 December 2024</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article></main><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12" id=to-top hidden=true><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div><footer class="py-10 print:hidden"><nav class="pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex list-none flex-col sm:flex-row"><li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a href=/tags/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Tags</span></a></li><li class="group mb-1 text-end sm:mb-0 sm:me-7 sm:last:me-0"><a href=https://drive.google.com/file/d/1blFQIqhagkHfd3sqsjawCyPCvl0Evrtw/view title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">CV</span></a></li></ul></nav><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">Copyright  2025 Karan Shah. If you copy this, may your WiFi become unstable.</p></div><div class="flex flex-row items-center"><div class="me-14 cursor-pointer text-sm text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"><button id=appearance-switcher-0 type=button aria-label="appearance switcher"><div class="flex h-12 w-12 items-center justify-center dark:hidden" title="Switch to dark appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="hidden h-12 w-12 items-center justify-center dark:flex" title="Switch to light appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://masterskepticista.github.io/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>